{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion\n",
    "\n",
    "ðŸ“œ [Deep Unsupervised Learning Using Non-equilibrium Thermodynamics](https://arxiv.org/pdf/1503.03585)\n",
    "\n",
    "> A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable.\n",
    "\n",
    "This captures the challenge that all the different probabilistic/generative models have to solve.\n",
    "\n",
    "> The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data.\n",
    "\n",
    "> Historically, probabilistic models suffer from a tradeoff between two conflicting objectives: tractability and flexibility.\n",
    "\n",
    "**1. Diffusion Probabilistic Models**\n",
    "\n",
    "> We present a novel way to define probabilistic models that allows:\n",
    "\n",
    "1. extreme flexibility in model structure\n",
    "2. exact sampling\n",
    "3. easy multiplication with other distributions, e.g. in order to compute a posterior, and\n",
    "4. the model log likelihood, and the probability of individual states, to be cheaply evaluated\n",
    "   >\n",
    "\n",
    "> Our method uses a Markov chain to gradually convert one distribution into another, an idea used in non-equilibrium statistical physics.\n",
    "\n",
    "The diffusion process slowly converts distributions from more noisy to more structured forms.\n",
    "\n",
    "> Since a diffusion process exists for any smooth target distribution, this method can capture data distributions of arbitrary form.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "> Our goal is to define a forward (or inference) diffusion process which converts any complex data distribution into a simple, tractable, distribution, and then learn a finite-time reversal of this diffusion process which defines our generative model distribution.\n",
    "\n",
    "**1. Forward Trajectory**\n",
    "\n",
    "> The data distribution is gradually converted into a well behaved distribution $\\pi (y)$ by repeated application of a Markov diffusion kernel $T_\\pi(y|yâ€™; \\beta)$ for $\\pi(y)$ where $\\beta$ is the diffusion rate.\n",
    "\n",
    "```math\n",
    "\\pi(y) = \\int dy' T_\\pi(y|y';\\beta)\\pi(y') \\\\\n",
    "\n",
    "q(x^{(t)}|x^{(t-1)}) = T_\\pi(x^{(t)}|x^{(t-1)}; \\beta_t)\n",
    "```\n",
    "\n",
    "> The forward trajectory corresponding to starting at the data distribution and performing $T$ steps of diffusion is thus\n",
    "\n",
    "```math\n",
    "q(x^{(0...T)}) = q(x^{(0)}) \\prod_{t=1}^T q(x^{(t)}|x^{(t-1)})\n",
    "```\n",
    "\n",
    "**2. Reverse Trajectory**\n",
    "\n",
    "> The generative distribution will be trained to describe the same trajectory, but in reverse.\n",
    "\n",
    "```math\n",
    "p(x^{(T)}) = \\pi(x^{(T)}) \\\\\n",
    "p(x^{(0...T)} = p(x^{(T)}) \\prod_{t=1}^T p(x^{(t-1)}|x^{(t)})\n",
    "```\n",
    "\n",
    "**3. Model Probability**\n",
    "\n",
    "> The probability the generative model assigns to the data is\n",
    "\n",
    "```math\n",
    "p(x^{(0)}) = \\int dx^{(1...T)} p(x^{(0...T)})\n",
    "```\n",
    "\n",
    "In order to actually be able to calculate this integral\n",
    "\n",
    "> We can instead evaluate the relative probability of the forward and reverse trajectories, averaged over forward trajectories.\n",
    "\n",
    "```math\n",
    "p(x^{(0)}) = \\int dx^{(1...T)}q(x^{(1...T)}|x^{(0)}) \\cdot p(x^{(T)}) \\prod_{t=1}^T \\frac{p(x^{(t-1)}|x^{(t)})}{q(x^{(t)}|x^{(t-1)})}\n",
    "```\n",
    "\n",
    "**4. Training**\n",
    "\n",
    "Training amounts to maximizing the model log likelihood\n",
    "\n",
    "```math\n",
    "L = \\int dx^{(0)} q(x^{(0)}) \\log p(x^{(0)})\n",
    "```\n",
    "\n",
    "Here, we maximize the likelihood that the model $p$ generates the state $x^{(0)}$ from some noisy state, conditioned by the weight of the actual sample in the dataset $q(x^{(0)})$.\n",
    "\n",
    "> The derivation of this bound parallels the derivation of the log likelihood bound in variational Bayesian methods.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "> We train diffusion probabilistic models on a variety of continuous datasets, and a binary dataset. We then demonstrate sampling from the trained model and in-painting of missing data, and compare model performance against.\n",
    "\n",
    "![Screenshot 2024-05-18 at 3.10.57â€¯PM.png](../../images/Screenshot_2024-05-18_at_3.10.57_PM.png)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> We have introduced a novel algorithm for modeling probability distributions that enables exact sampling and evaluation of probabilities and demonstrated its effectiveness on a variety of toy and real datasets, including challenging natural image datasets.\n",
    "\n",
    "> The result is an algorithm that can learn a fit to any data distribution, but which remains tractable to train, exactly sample from, and evaluate, and under which it is straightforward to manipulate conditional and posterior distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion\n",
    "\n",
    "ðŸ“œ [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)\n",
    "\n",
    "> We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from non-equilibrium thermodynamics.\n",
    "\n",
    "> Our best results are obtained by training on a weighted variational\n",
    "> bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics.\n",
    "\n",
    "> And our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding.\n",
    "\n",
    "> We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models.\n",
    "\n",
    "> We find that the majority of our modelsâ€™ lossless code-lengths are consumed to describe imperceptible image details.\n",
    "\n",
    "> We show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.\n",
    "\n",
    "### Background\n",
    "\n",
    "â€¦\n",
    "\n",
    "### Diffusion Models and Denoising Autoencoders\n",
    "\n",
    "> To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching that leads to a simplified, weighted variational bound objective for diffusion models.\n",
    "\n",
    "**1. Forward Process and $L_T$**\n",
    "\n",
    "> In our implementation, the approximate posterior $q$ has no learnable parameters, so $L_T$ is a constant during training and can be ignored.\n",
    "\n",
    "â€¦\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> We have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
