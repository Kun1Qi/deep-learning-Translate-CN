{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN\n",
    "\n",
    "ðŸ“œ [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661v1)\n",
    "\n",
    "> We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.\n",
    "\n",
    "> This framework corresponds to a minimax two-player game.\n",
    "\n",
    "> Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context.\n",
    "\n",
    "### Adversarial Nets\n",
    "\n",
    "> We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $\\log(1 âˆ’ D(G(z)))$\n",
    "\n",
    "> In other words, $D$ and $G$ play the following two-player minimax game with value function $V(G,D)$\n",
    "\n",
    "$$\n",
    "\\underset{G}{\\textrm{min}} \\, \\underset{D}{\\textrm{max}} \\,\n",
    "V(D,G) = \\mathbb{E}_{x \\sim p_{\\textrm{data}}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_x(z)} [\\log(1 - D(G(z))]\n",
    "$$\n",
    "\n",
    "> Rather than training G to minimize $\\log(1 âˆ’ D(G(z)))$ we can train $G$ to maximize $\\log D(G(z))$. This objective function results in the\n",
    "> same fixed point of the dynamics of $G$ and $D$ but provides much stronger gradients early in learning.\n",
    "\n",
    "Allowing the generator to maximize the probability of fooling the discriminator, rather than minimize the probability of getting discovered by the discriminator, provides a better optimization since itâ€™s very easy for the discriminator to vote against the generator early on.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "![Screenshot 2024-05-18 at 2.04.41â€¯PM.png](../../images/Screenshot_2024-05-18_at_2.04.41_PM.png)\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "> The disadvantages are primarily that there is no explicit representation of $p_g(x)$, and that $D$ must be synchronized well with $G$ during training (in particular, G must not be trained too much without updating D).\n",
    "\n",
    "> The advantages are that Markov chains are never needed, only back-prop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model.\n",
    "\n",
    "Generative adversarial models are far more computationally efficient than the previous Markov chain based models.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
