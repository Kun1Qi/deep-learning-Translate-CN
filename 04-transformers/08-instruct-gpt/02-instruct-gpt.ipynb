{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "Earlier GPT models like GPT-2 and GPT-3 continued to break state-of-the-art in language modeling tasks, but still didn't yet break through into the mainstream. The models were powerful, but not yet practically usable for most people.\n",
    "\n",
    "The introduction of InstructGPT completely changed this, as it triggered the trajector of assistant models that led to ChatGPT, which instantly blew up and start the current cycle of awareness and hype around AI.\n",
    "\n",
    "InstructGPT shows the impacts of effective fine-tuning - they creative a human generated dataset of good output behaviors for an assistant model, then trained a reward model to predict human outputs and used it as a function to maximize reward for PPO.\n",
    "\n",
    "Notably, the model 1.3B model with this fine-tuning result had outputs preferred by humans over the outputs of the 100x larger 175B parameter GPT-3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Notes\n",
    "\n",
    "📜 [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)\n",
    "\n",
    "> Making language models bigger does not inherently make them better at following a user’s intent.\n",
    "\n",
    "> In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.\n",
    "\n",
    "> In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation.\n",
    "\n",
    "> Our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.\n",
    "\n",
    "> We want language models to be _helpful_, _honest_, and _harmless_.\n",
    "\n",
    "> We focus on fine-tuning approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF) to fine-tune GPT-3 to follow a broad class of written instructions. This technique uses human preferences as a reward signal to fine-tune our models.\n",
    "\n",
    "Using RLHF to align the model with human preferences via fine-tuning.\n",
    "\n",
    "> We first hire a team of 40 contractors to label our data, based on their performance on a screening test.\n",
    "\n",
    "This is still an extremely manual and human involved process.\n",
    "\n",
    "The procedure is:\n",
    "(1) collect a dataset of human-written desired output behaviors and some labeler written prompts and use this to train GPT-3.\n",
    "(2) next, collect a dataset of human-labeled comparisons between outputs, then train a reward model (RM) to predict which outputs labelers prefer.\n",
    "(3) then use the RM as a reward function to maximize reward for the model using PPO.\n",
    "\n",
    "> This procedure aligns the behavior of GPT-3 to the stated preferences of a specific group of people (mostly our labelers and researchers), rather than any broader notion of “human values”. We call the resulting models InstructGPT.\n",
    "\n",
    "### Methods and experimental details\n",
    "\n",
    "> Step 1: Collect demonstration data, and train a supervised policy\n",
    "> Step 2: Collect comparison data, and train a reward model\n",
    "> Step 3: Optimize a policy against the reward model using PPO\n",
    "\n",
    "### Results\n",
    "\n",
    "**1. Results on the API distribution**\n",
    "\n",
    "> Labelers significantly prefer InstructGPT outputs over outputs from GPT-3\n",
    "\n",
    "> Our models generalize to the preferences of “held-out” labelers that did not produce any training data.\n",
    "\n",
    "> Public NLP datasets are not reflective of how our language models are used.\n",
    "\n",
    "**2. Results on public NLP datasets**\n",
    "\n",
    "> InstructGPT models show improvements in truthfulness over GPT-3\n",
    "\n",
    "> We can minimize performance regressions on public NLP datasets by modifying our fine-tuning procedure.\n",
    "\n",
    "**3. Qualitative Results**\n",
    "\n",
    "> InstructGPT models show promising generalization to instructions outside of the RLHF fine-tuning distribution.\n",
    "\n",
    "> InstructGPT still makes simple mistakes.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "**1. Implications for alignment research**\n",
    "\n",
    "> The cost of increasing model alignment is modest relative to pre-training.\n",
    "\n",
    "> We’ve seen some evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in.\n",
    "\n",
    "> We were able to mitigate most of the performance degradations introduced by our fine-tuning.\n",
    "\n",
    "> We’ve validated alignment techniques from research in the real world.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
