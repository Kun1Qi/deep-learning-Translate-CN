{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "Included for it's historical significance along with the GPT-3 paper. These introduce almost nothing new in the papers themselves, but they start to show how correct the scaling laws are, and how far they go.\n",
    "\n",
    "The GPT series improves on the progress made by BERT, RoBERTa, T5, and more (the original GPT actually came before all of these as well).\n",
    "\n",
    "Most important, it shows that as we scale up parameters and data, the model base intelligence gets better, and starts to beat state of the art in more and more tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Notes\n",
    "\n",
    "ðŸ“œ [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "> We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.\n",
    "\n",
    "Language models start to learn and be able to complete tasks that typically required fine-tuning and supervised learning in an unsupervised way if they are just given enough data to train on.\n",
    "\n",
    "> Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still under-fits WebText.\n",
    "\n",
    "> Our suspicion is that the prevalence of single task training on single domain datasets is a major contributor to the lack of generalization observed in current systems.\n",
    "\n",
    "> It will be very difficult to continue to scale the creation of datasets and the design of objectives to the degree that may be required to brute force our way there with current techniques. This motivates exploring additional setups for performing multitask learning.\n",
    "\n",
    "Multitask learning is promising for approaching general intelligence in language models, but it is expensive to create labeled datasets for it.\n",
    "\n",
    "> We demonstrate language models can perform down-stream tasks in a zero-shot setting â€“ without any parameter or architecture modification.\n",
    "\n",
    "### Approach\n",
    "\n",
    "> Language modeling is also able to, in principle, learn the tasks of without the need for explicit supervision of which symbols are the outputs to be predicted.\n",
    "\n",
    "> The internet contains a vast amount of information that is passively available without the need for interactive communication. Our speculation is that a language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement.\n",
    "\n",
    "Hereâ€™s the hypothesis of OpenAI that leads to all their scaling laws research. The intuition is that the internet already has a ton of data and that providing the model with this data will make it learn more than people expect.\n",
    "\n",
    "**1. Training Dataset**\n",
    "\n",
    "> Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.\n",
    "> A promising source of diverse and nearly unlimited text is web scrapes such as Common Crawl.\n",
    "\n",
    "> Instead, we created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been curated/filtered by humans.\n",
    "\n",
    "We see improvements in taste on the dataset (determined by humans) improving the quality of the model. And the broader trend of improving the quality of web scrapes and the model size to improve the model itself.\n",
    "\n",
    "**2. Input Representation**\n",
    "\n",
    "> We prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "> Our largest model, which we call GPT-2, has over an order of magnitude more parameters than GPT.\n",
    "\n",
    "![Screenshot 2024-05-16 at 11.59.59â€¯AM.png](../../images/Screenshot_2024-05-16_at_11.59.59_AM.png)\n",
    "\n",
    "### Discussion\n",
    "\n",
    "> Much research has been dedicated to learning, understanding, and critically evaluating the representations of both supervised and unsupervised pre-training methods. Our results suggest that unsupervised task learning is an additional promising area of research to explore.\n",
    "\n",
    "> While zero-shot performance establishes a baseline of the potential performance of GPT-2 on many tasks, it is not clear where the ceiling is with fine-tuning.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
