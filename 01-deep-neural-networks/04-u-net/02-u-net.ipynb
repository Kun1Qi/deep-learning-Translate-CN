{"cells":[{"cell_type":"markdown","metadata":{"id":"lTchKM0wqVf9"},"source":["# Explanation\n","\n","U-Net takes advantage of the CNN architecture and data augmentation to create a highly effective image segmentation model with very little data. This is interesting because deep learning usually takes a lot of data - but U-Net is an example of solving a challenging problem with inductive bias, despite low data.\n","\n","I've included U-Net here not just for it's independent value but also its practical value - it's now the model of choice in many modern image generation architectures we'll cover in section 5."]},{"cell_type":"markdown","metadata":{"id":"W11BaqtlqohC"},"source":["# My Notes\n","\n","ðŸ“œ [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597)\n","\n","\n","> There is large consent that successful training of deep networks  requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.\n","> \n","\n","This paper shows us that itâ€™s possible to train effective deep neural networks even with a small amount of data, given their new approaches.\n","\n","> The architecture consists of a contracting path to capture\n","context and a symmetric expanding path that enables precise localization.\n","> \n","\n","> We show that such a network can be trained end-to-end from very\n","few images and outperforms the prior best method.\n","> \n","\n","> The main idea in to supplement a usual contracting network by\n","successive layers, where pooling operators are replaced by upsampling operators.\n","> \n","\n","> In order to localize, high resolution features from the contracting path are combined with the upsampled output.\n","> \n","\n","> One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers.\n","> \n","\n","> As for our tasks there is very little training data available, we use excessive\n","data augmentation by applying elastic deformations to the available training images.\n","> \n","\n","### Network Architecture\n","\n","![Screenshot 2024-05-22 at 2.48.43â€¯AM.png](../../images/Screenshot_2024-05-22_at_2.48.43_AM.png)\n","\n","### Training\n","\n","**3.1 Data Augmentation**\n","\n","> We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid.\n","> \n","\n","They use data augmentation to successfully modify the dataset and make it much larger, letting them improve the model with very few available training samples.\n","\n","### Experiments\n","\n","![Screenshot 2024-05-22 at 3.02.05â€¯AM.png](../../images/Screenshot_2024-05-22_at_3.02.05_AM.png)\n","\n","### Conclusion\n","\n","> Thanks to data augmentation with elastic deformations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVIDIA Titan GPU\n",">"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNr2Fce2xudaCqxgQPafuCi","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
