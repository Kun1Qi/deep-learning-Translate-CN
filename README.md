# deep-learning [WIP]

An exploration of the entire history of deep learning from DNNs to GPT-4o through the lens of the constraints limiting progress.

In the repository, I've included a copy of each critical paper (40+), along with my full explanation of high-level intuitions, mathematical intuitions, code implementation, and broader impact.

On the rest of this page, I provide an overview of everything we can learn from this history, inspired by [_The Lessons of History_](https://www.amazon.com/Lessons-History-Will-Durant/dp/143914995X) by Will & Ariel Durant.

**The project is designed so everyone can get most of the value by reading the overview below.**

Then, people curious to dive into the technical details can explore the rest of the repository via the links in the [resources](#resources) section.

## Table of Contents

- [Overview](#overview-the-lessons-of-history)
  - [1. Introduction](#1-introduction)
  - [2. Constraints](#2-constraints)
    - [2.1. Data](#21-data)
    - [2.2. Size](#22-size)
    - [2.3. Optimization & Regularization](#23-optimization--regularization)
    - [2.4. Architecture](#24-architecture)
    - [2.5. Compute](#25-compute)
    - [2.6. Compute Efficiency](#26-compute-efficiency)
    - [2.7. Energy](#27-energy)
    - [2.8. Constraints & Leverage](#28-constraints--leverage)
  - [3. Narratives](#3-narratives)
  - [4. Inspiration](#4-inspiration)
  - [5. Intelligence](#5-intelligence)
  - [6. Future](#6-future)
- [Resources](#part-2-the-arrow-of-progress)

# Overview

_Format inspired by [The Lessons of History](https://www.amazon.com/Lessons-History-Will-Durant/dp/143914995X) by Will and Ariel Durant_

## 1. Introduction

## 2. Constraints

### 2.1. Data

### 2.2. Size

### 2.3. Optimization & Regularization

### 2.4. Architecture

### 2.5. Compute

### 2.6. Compute Efficiency

### 2.7. Energy

### 2.8. Constraints & Leverage

## 3. Narratives

## 4. Inspiration

## 5. Intelligence

## 6. Future

# Resources

> [!IMPORTANT]
>
> Each technical breakthroughs highlighted in this repository is covered in a folder linked below.
>
> In each folder, you'll find a copy of the relevant papers (`.pdf` files), along with my own breakdown of intuitions, math, impact, and my implementation when relevant (all in the `.ipynb` file).

**1. Deep Neural Networks**

- [1.1. DNN](/01-deep-neural-networks/01-dnn/)
- [1.2. CNN](/01-deep-neural-networks/02-cnn/)
- [1.3. AlexNet](/01-deep-neural-networks/03-alex-net/)
- [1.4. UNet](/01-deep-neural-networks/04-u-net/)

**2. Optimization & Regularization**

- [2.1. Weight Decay](/02-optimization-and-regularization/01-weight-decay/)
- [2.2. ReLU](/02-optimization-and-regularization/02-relu/)
- [2.3. Residuals](/02-optimization-and-regularization/03-residuals/)
- [2.4. Dropout](/02-optimization-and-regularization/04-dropout/)
- [2.5. Batch Normalization](/02-optimization-and-regularization/05-batch-norm/)
- [2.6. Layer Normalization](/02-optimization-and-regularization/06-layer-norm/)
- [2.7. GELU](/02-optimization-and-regularization/07-gelu/)
- [2.8. Adam](/02-optimization-and-regularization/08-adam/)

**3. Sequence Modeling**

- [3.1. RNN](/03-sequence-modeling/01-rnn/)
- [3.2. LSTM](/03-sequence-modeling/02-lstm/)
- [3.3. Learning to Forget](/03-sequence-modeling/03-learning-to-forget/)
- [3.4. Word2Vec](/03-sequence-modeling/04-word2vec/)
- [3.5. Encoder-Decoder](/03-sequence-modeling/05-encoder-decoder/)
- [3.6. Seq2Seq](/03-sequence-modeling/06-seq2seq/)
- [3.7. Attention](/03-sequence-modeling/07-attention/)
- [3.8. Mixture of Experts](/03-sequence-modeling/08-mixture-of-experts/)

**4. Transformers**

- [4.1. Transformer](/04-transformers/01-transformer/)
- [4.2. BERT](/04-transformers/02-bert/)
- [4.3. RoBERTa](/04-transformers/03-roberta/)
- [4.4. T5](/04-transformers/04-t5/)
- [4.5. GPT-2](/04-transformers/05-gpt-2/)
- [4.6. GPT-3](/04-transformers/06-gpt-3/)
- [4.7. LoRA](/04-transformers/07-lora/)
- [4.8. InstructGPT](/04-transformers/08-instruct-gpt/)
- [4.9. Vision Transformer](/04-transformers/09-vision-transformer/)
- [4.10. Mixture of Experts Transformer](/04-transformers/10-moe-transformer/)

**5. Image Generation**

- [5.1. GANs](/05-image-generation/01-gan/)
- [5.2. VAEs](/05-image-generation/02-vae/)
- [5.3. Diffusion](/05-image-generation/03-diffusion/)
- [5.4. Stable Diffusion, ControlNet, & SDXL](/05-image-generation/04-stable-diffusion/)
- [5.5. CLIP](/05-image-generation/05-clip/)
- [5.6. DALL E & DALL E 2](/05-image-generation/06-dall-e/)
