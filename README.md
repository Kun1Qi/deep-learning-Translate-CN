# deep-learning
  æ·±åº¦å­¦ä¹ 

A deep-dive on the entire history of deep learning, highlighting the series of innovations that got us from simple feed-forward networks to GPT-4o.

è¿™é‡Œæ·±å…¥äº†æ•´ä¸ªæ·±åº¦å­¦ä¹ çš„å†å²ï¼Œé‡ç‚¹æåˆ°äº†ä¸€ç³»åˆ—çš„åˆ›æ–°ï¼Œå…¶ä½¿å¾—æˆ‘ä»¬ä»ç®€å•çš„å‰å‘ç½‘ç»œç›´åˆ°ä»Šå¤©çš„GPT-4oã€‚

For each key milestone, I've included the critical papers in this repository, along with my notes, my explanation of important intuitions & math, and a toy implementation in pytorch when relevant.

å¯¹äºæ¯ä¸€ä¸ªå…³é”®çš„é‡Œç¨‹ç¢‘ï¼Œæˆ‘ä»¬å·²ç»åŒ…å«äº†å…³é”®çš„è®ºæ–‡åœ¨è¿™ä¸ªä»“åº“ä¸­ï¼ŒåŒ…æ‹¬æˆ‘çš„è§‚ç‚¹ï¼Œç›´è§‚å’Œæ•°å­¦ä¸Šçš„è§£é‡Šï¼Œä»¥åŠå¯¹ç›¸å…³å†…å®¹çš„pytorchå®ç°çš„å°æ¡ˆä¾‹ã€‚

The rest of this page is my breakdown of everything we can learn from this history, and what it tells us about the future of deep learning, inspired by [_The Lessons of History_](https://www.amazon.com/Lessons-History-Will-Durant/dp/143914995X) by Will & Ariel Durant.

è¿™ä¸€é¡µçš„å‰©ä½™éƒ¨åˆ†æ˜¯åŸºäºæˆ‘å¯¹è¿™æ®µå†å²çš„ä¸€ä¸ªåˆ’åˆ†ï¼Œä»¥åŠå¯¹äºæ·±åº¦å­¦ä¹ æœªæ¥çš„ä¸€äº›æƒ³æ³•ï¼Œè¿™æ˜¯å—åˆ°[_The Lessons of History_]çš„å¯å‘ã€‚

> [!IMPORTANT]
> é‡ç‚¹
> 
> **This project is designed so everyone can get most of the value by just reading my overview on the rest of this page.**
> 
> è¿™ä¸ªé¡¹ç›®æ˜¯è®¾è®¡ç»™æ¯ä¸€ä¸ªäººå»å‘ç°æœ€å¤§ä»·å€¼ï¼Œä½†è¿™éœ€è¦å»é˜…è¯»æœ¬é¡µå‰©ä½™éƒ¨åˆ†çš„è§‚ç‚¹
> 
> Then, people curious to learn about the technical details of each innovation can explore the rest of the repository via the links in the [resources](#resources) section.
> 
> ç„¶åï¼Œæœ‰å…´è¶£çš„äººæƒ³è¦å­¦ä¹ æ¯ä¸€ä¸ªåˆ›æ–°çš„æŠ€æœ¯ç»†èŠ‚ï¼Œå¯ä»¥çœ‹ä¸€ä¸‹ä»“åº“çš„å‰©ä½™éƒ¨åˆ†ï¼Œå¯ä»¥é€šè¿‡ [resources](#resources)ç›´è¾¾ã€‚

> [!NOTE]
> æ³¨æ„
> 
> Thanks to [Pavan Jayasinha](https://x.com/pavanjayasinha) and [Anand Majmudar](https://x.com/Almondgodd) for their constant feedback while I made this ğŸ˜„
> 
> æ„Ÿè°¢ [Pavan Jayasinha](https://x.com/pavanjayasinha) and [Anand Majmudar](https://x.com/Almondgodd) çš„æ—¥å¸¸åé¦ˆå¯¹æˆ‘çš„å¸®åŠ©ã€‚

## Table of Contents

- [Overview](#overview)
- æ¦‚è§ˆ
  - [1. Constraints](#2-constraints)
  - 1.çº¦æŸ
    - [1.1. Data](#11-data)
    - 1.1 æ•°æ®
    - [1.2. Parameters](#12-parameters)
    - 1.2 å‚æ•°
    - [1.3. Optimization & Regularization](#13-optimization--regularization)
    - 1.4 ä¼˜åŒ–å’Œæ­£åˆ™åŒ–
    - [1.4. Architecture](#14-architecture)
    - 1.4 æ¶æ„
    - [1.5. Compute](#15-compute)
    - 1.5 è®¡ç®—
    - [1.6. Compute Efficiency](#16-compute-efficiency)
    - 1.6 è®¡ç®—æ•ˆç‡
    - [1.7. Energy](#17-energy)
    - 1.7 èƒ½æº
    - [1.8. Constraints & Leverage](#18-constraints--leverage)
    - 1.8 çº¦æŸå’Œåº”ç”¨
  - [2. Narratives](#2-narratives)
  - 2.ä¸€äº›å™è¿°
  - [3. Inspiration](#3-inspiration)
  - 3.å¯å‘
  - [4. Intelligence](#4-intelligence)
  - 4.æ™ºèƒ½
  - [5. Future](#5-future)
  - 5.æœªæ¥
- [Resources](#resources)
- èµ„æº
  - [Topics](#topics)
  - ä¸»é¢˜
  - [Implementations](#implementations)
  - å®ç°
  - [Papers](#papers)
  - è®ºæ–‡

<br />

# Overview
  æ¦‚è§ˆ
  
The most interesting part of my deep-dive came from noticing a clear trend across all the key advancements, which has completely reframed how I understand deep learning:

æˆ‘è¿™é‡Œæ‰€æ·±å…¥çš„æœ€æœ‰æ„æ€çš„éƒ¨åˆ†æ˜¯æ¥è‡ªå¯¹æ‰€æœ‰å…³é”®è¿›å±•å’Œè¶‹åŠ¿çš„æ¸…æ¥šè®ºè¿°ï¼Œè¿™å®Œå…¨é‡æ„äº†æˆ‘çš„æ·±åº¦å­¦ä¹ çš„ç†è§£ã€‚

> [!IMPORTANT]
> é‡ç‚¹
> 
> **There are 7 simple constraints that limit the capacity of digital intelligence:**
>
> è¿™é‡Œæœ‰7ä¸ªç®€å•çš„çº¦æŸé™åˆ¶äº†æ•°å­—æ™ºèƒ½çš„èƒ½åŠ›ã€‚
> 1. data
>    æ•°æ®
> 2. parameters
>    å‚æ•°
> 3. optimization & regularization
>    ä¼˜åŒ–å’Œæ­£åˆ™åŒ–
> 4. architecture
>    æ¶æ„
> 5. compute
>    è®¡ç®—
> 6. compute efficiency
>    è®¡ç®—æ•ˆç‡
> 7. energy
>    èƒ½æº
>
> **The entire history of deep learning can be seen as the series of advancements that have gradually raised the ceiling on these constraints**, enabling the creation of increasingly intelligent systems.
> æ•´ä¸ªæ·±åº¦å­¦ä¹ çš„å†å²èƒ½è¢«çœ‹ä½œä¸€ç³»åˆ—çš„æå‡ï¼Œå·²ç»é€æ¸åˆ°è¾¾è¿™äº›çº¦æŸçš„ä¸Šé™ï¼Œä½¿å¾—èƒ½æ„å»ºé€æ¸æ™ºèƒ½åŒ–çš„ç³»ç»Ÿã€‚

It's impossible to understand where we're going without first understanding how we got here - and it's impossible to understand how we got here without understanding these constraints, which have always governed the rate of progress.
å¦‚æœæˆ‘ä»¬ä¸é¦–å…ˆç†è§£æ‰€å¤„çš„ä½ç½®ï¼Œå°†æ— æ³•ç†è§£è¦å»å“ªé‡Œâ€”â€”åŒæ—¶ï¼Œä¹Ÿä¸å¯èƒ½ä¸äº†è§£è¿™äº›çº¦æŸå°±èƒ½çŸ¥é“æˆ‘ä»¬çš„ç°çŠ¶ï¼Œå› ä¸ºè¿™äº›çº¦æŸä¸€ç›´å½±å“ç€è¿›æ­¥çš„æ•ˆç‡ã€‚

By understanding them, we can also explore a few related questions:
é€šè¿‡è¿™äº›ç†è§£ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½æ¢ç´¢ä¸€äº›æ–°çš„ç›¸å…³é—®é¢˜ï¼š

- How is progress made in deep learning?
- è¿™äº›æ·±åº¦å­¦ä¹ çš„è¿›æ­¥å¦‚ä½•åšåˆ°çš„ï¼Ÿ
- Where do the ideas that drive progress in deep learning come from?
- è¿™äº›æƒ³æ³•å“ªé‡Œæ¥çš„ï¼Œèƒ½é©±åŠ¨æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Ÿ
- How have our narratives about digital intelligence changed over time?
- æˆ‘ä»¬å¦‚ä½•è®ºè¿°æ•°å­—æ™ºèƒ½éšç€æ—¶é—´çš„å˜åŒ–ï¼Ÿ
- What does deep learning teach us about our own intelligence?
- æ·±åº¦å­¦ä¹ èƒ½å¯¹æˆ‘ä»¬è‡ªèº«æ‹¥æœ‰çš„æ™ºèƒ½æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ
- Where is the future of deep learning headed?
- æ·±åº¦å­¦ä¹ æœªæ¥çš„æ–¹å‘åœ¨å“ªé‡Œï¼Ÿ

So, let's start by understanding these constraints from first principles.
è¿™æ ·ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç†è§£è¿™äº›çº¦æŸï¼ŒåŸºäºç¬¬ä¸€æ€§åŸç†ã€‚

<br />

# 1. Constraints
  1.çº¦æŸ

We can define intelligence[^1] as the ability to accurately model reality[^2]. Practically, we're interested in models of reality that are useful for performing economically valuable tasks.

æˆ‘ä»¬èƒ½å®šä¹‰æ™ºèƒ½ä¸ºä¸€ç§èƒ½åŠ›ï¼Œå¯ä»¥å‡†ç¡®çš„å»ºæ¨¡ç°å®ä¸–ç•Œã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å…³æ³¨ç°å®ä¸–ç•Œçš„å»ºæ¨¡ï¼Œå¯ä»¥ç”¨æ¥å®ç°ä¸€äº›æœ‰ç»æµä»·å€¼çš„ä»»åŠ¡ã€‚

The goal of deep learning is to produce accurate models of reality for these useful tasks by:

æ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ˜¯å»ç”Ÿæˆå‡†ç¡®çš„ç°å®ä¸–ç•Œçš„æ¨¡å‹ç”¨äºè¿™äº›ä»»åŠ¡ï¼š

1. Treating the true models that describe reality as complex probability distributions[^3]

   ç”¨æ­£ç¡®çš„æ¨¡å‹æ¥æè¿°çœŸå®çš„ä¸–ç•Œï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒã€‚
2. Creating neural networks capable of modeling complex probability distributions

   åˆ›å»ºç¥ç»ç½‘ç»œçš„èƒ½åŠ›ï¼Œå»å»ºæ¨¡å¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒã€‚
3. Training these networks to learn to model the probability distributions that underlie reality

   è®­ç»ƒè¿™äº›ç½‘ç»œå»å­¦ä¹ å»ºæ¨¡æ¦‚ç‡åˆ†å¸ƒï¼Œèƒ½è¡¨å¾çœŸå®ä¸–ç•Œã€‚ 

In this view, creating intelligence with deep learning involves just two steps:

ä»è¿™ä¸ªè§†è§’ï¼Œåˆ›å»ºæ™ºèƒ½ç”¨åˆ°æ·±åº¦å­¦ä¹ ï¼Œåªæ˜¯æ¶‰åŠä¸¤ä¸ªæ­¥éª¤ï¼š

1. Collect useful information about reality (collect data)

   æ”¶é›†æœ‰ç”¨çš„å…³äºçœŸå®ä¸–ç•Œçš„ä¿¡æ¯ï¼ˆæ”¶é›†æ•°æ®ï¼‰
2. Create a neural network that can effectively learn from this information (model data)

    åˆ›å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œèƒ½æœ‰æ•ˆçš„å­¦ä¹ è¿™äº›ä¿¡æ¯ï¼ˆæ¨¡å‹æ•°æ®ï¼‰

The only way to increase the intelligence of our model is to improve how well we accomplish each of these steps.

å”¯ä¸€çš„æ–¹æ³•å»æå‡æˆ‘ä»¬æ¨¡å‹çš„æ™ºèƒ½ï¼Œå°±æ˜¯å»æå‡æˆ‘ä»¬è¦å®ç°çš„æ¯ä¸€ä¸ªæ­¥éª¤ã€‚

With this in mind, we can look at the constraints that govern this process. Let's start by understanding the constraint on data.

åŸºäºè¿™ä¸ªæƒ³æ³•ï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°è¿™äº›çº¦æŸæ¶‰åŠäº†è¿™ä¸ªè¿‡ç¨‹ã€‚è®©æˆ‘ä»¬å¼€å§‹ç†è§£æ•°æ®ä¸Šçš„è¿™äº›çº¦æŸã€‚

<br />
[^1]: Everyone has different definitions of intelligence, all of which are useful in different contexts, and none of which capture the full picture of what this word means. People may disagree with the specifics of this definition. I've chosen this one for the sake of simplicity to clearly frame what we're trying to achieve with deep learning from an economic perspective- I'm less concerned with it's philosophical implications here.

 æ¯ä¸€ä¸ªäººå¯¹æ™ºèƒ½æœ‰ä¸åŒçš„å®šä¹‰ï¼Œæ‰€æœ‰è¿™äº›ä¸åŒçš„å®šä¹‰éƒ½åŸºäºä¸åŒçš„é¢†åŸŸï¼Œä½†æ˜¯éƒ½æ— æ³•å®Œæ•´çš„æ¦‚æ‹¬è¿™ä¸ªè¯çš„å«ä¹‰ã€‚äººä»¬å¯èƒ½ä¸åŒæ„è¿™ä¸ªå®šä¹‰çš„è®ºè¿°ã€‚æˆ‘å·²ç»æœ‰ç›®çš„é€‰æ‹©äº†ä¸€ä¸ªç®€å•æ¸…æ¥šçš„æè¿°ï¼Œå°±æ˜¯å°è¯•å»ç”¨æ·±åº¦å­¦ä¹ å®ç°ç»æµè§’åº¦çš„ç›®æ ‡ã€‚æˆ‘åœ¨è¿™é‡Œæ²¡æœ‰å…³æ³¨å…¶å“²å­¦çš„æ„ä¹‰ã€‚
[^2]: Karl Friston's [Free Energy Principle](https://www.nature.com/articles/nrn2787) suggests that this definition of intelligence is also valid in the context of the brain (beware, the paper is explained with unnecessary mathematical complexity, but the core concept it describes is simple). Notably, intelligence systems create models of the world and then use those models to perform _active inference_ to modify their environments.

 Karl Fristonçš„è‡ªç”±èƒ½é‡æ³•åˆ™è®ºè¿°çš„è¿™ä¸ªæ™ºèƒ½çš„å®šä¹‰ï¼Œä¹Ÿå¯ä»¥ç”¨åœ¨å¤§è„‘ç›¸å…³çš„å†…å®¹ä¸­ï¼ˆå½“å¿ƒï¼Œè¿™ç¯‡è®ºæ–‡çš„äº†ä¸­åŒ…å«äº†ä¸å¿…è¦çš„å¤æ‚æ•°å­¦ï¼Œä½†æ˜¯ï¼Œæ ¸å¿ƒæ¦‚å¿µçš„è®ºè¿°æ˜¯ç®€æ´çš„ï¼‰ã€‚æ³¨æ„ï¼Œæ™ºèƒ½ç³»ç»Ÿåˆ›å»ºä¸–ç•Œæ¨¡å‹ï¼Œå¹¶ç”¨è¿™äº›æ¨¡å‹å»åšæ´»åŠ¨çš„æ¨ç†ï¼Œæ¥æ”¹å˜å…¶ç¯å¢ƒã€‚
[^3]: This idea may seem unintuitive at first. But it's actually saying something very simple: (1) reality has a set of rules that govern what happens (2) we can model these rules by assigning probabilities to what's likely to happen, given what has already happened (3) thus, these models are probability distributions. Again, the [Free Energy Principle](https://www.nature.com/articles/nrn2787) supports this view of modeling reality.
 
  è¿™ä¸ªæƒ³æ³•é¦–å…ˆçœ‹èµ·æ¥ä¸å¤Ÿç›´è§‚ã€‚ä½†æ˜¯å…¶å®é™…è®ºè¿°çš„äº‹æƒ…å¾ˆç®€å•ï¼šï¼ˆ1ï¼‰ç°å®ä¸–ç•Œæœ‰ä¸€ç»„è§„åˆ™èƒ½æ§åˆ¶ä»€ä¹ˆä¼šå‘ç”Ÿï¼ˆ2ï¼‰æˆ‘ä»¬èƒ½å»ºæ¨¡è¿™äº›è§„åˆ™ï¼Œç”¨äº†åˆ†é…æ¦‚ç‡çš„æ–¹å¼ï¼Œå»å®šä¹‰ä»€ä¹ˆè¦å‘ç”Ÿï¼Œé‡åŒ–å·²ç»å‘ç”Ÿçš„ï¼ˆ3ï¼‰å› æ­¤ï¼Œè¿™äº›æ¨¡å‹éƒ½æ˜¯æ¦‚ç‡åˆ†å¸ƒã€‚è€Œä¸”ï¼Œè‡ªç”±èƒ½é‡æ³•åˆ™æ”¯æŒè¿™ä¸ªå»ºæ¨¡çœŸå®ä¸–ç•Œçš„è§‚ç‚¹ã€‚

## 1.1. Data
   1.1. æ•°æ®

![constraint-1-data](./images/readme/constraint-1-data.png)

We've established that the goal of deep learning is to model the probability distributions that describe reality.
æˆ‘ä»¬å·²ç»æ„å»ºäº†æ·±åº¦å­¦ä¹ çš„ç›®æ ‡ï¼Œå°±æ˜¯å»å»ºæ¨¡æ¦‚ç‡åˆ†å¸ƒå»è¡¨è¾¾çœŸå®çš„ä¸–ç•Œã€‚

Let's call the distribution that we're trying to model for a specific task the _true distribution_. In order to learn about the true distribution, we collect many samples from it. These samples make up a _dataset_.
æˆ‘ä»¬æŠŠè¿™äº›å°è¯•å»ºæ¨¡ä¸€ä¸ªç‰¹å®šä»»åŠ¡çš„åˆ†å¸ƒç§°ä¸ºçœŸå®çš„åˆ†å¸ƒã€‚ä¸ºäº†å»å­¦ä¹ è¿™ä¸ªçœŸå®çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬åœ¨å…¶ä¹‹ä¸Šæ”¶é›†å¤§é‡çš„æ ·æœ¬ã€‚è¿™äº›æ ·æœ¬æ„æˆäº†ä¸€ä¸ªæ•°æ®é›†ã€‚

The dataset contains some information about the true distribution, but it doesn't contain _all_ information about the true distribution[^4]. Because of this, the dataset represents an approximation of the true distribution, which we'll 
è¿™ä¸ªæ•°æ®é›†åŒ…å«äº†ä¸€äº›ä¿¡æ¯æ˜¯å…³äºçœŸå®åˆ†å¸ƒçš„ï¼Œä½†æ˜¯æ²¡æœ‰åŒ…å«è¿™ä¸ªçœŸå®åˆ†å¸ƒçš„å…¨éƒ¨ä¿¡æ¯ã€‚å› æ­¤ï¼Œè¿™ä¸ªæ•°æ®é›†è¿‘ä¼¼çš„è¡¨å¾äº†è¿™ä¸ªçœŸå®åˆ†å¸ƒï¼Œè¢«æˆ‘ä»¬ç§°ä¸ºç»éªŒåˆ†å¸ƒã€‚
call the _empirical distribution_.

**At best, we can expect our neural network to learn to model this empirical distribution[^5].**
æœ€å¥½çš„æƒ…å†µï¼Œæˆ‘ä»¬èƒ½æœŸæœ›ç¥ç»ç½‘ç»œå»å­¦ä¹ å»ºæ¨¡è¿™ä¸ªç»éªŒåˆ†å¸ƒã€‚

However, our original goal was to model the true distribution. To account for this, we need the empirical distribution to be **a good approximation** of the true distribution. The quality of this approximation determines the cap of how 
ä½†æ˜¯ï¼Œæˆ‘ä»¬å¼€å§‹çš„ç›®æ ‡æ˜¯å»å»ºæ¨¡çœŸå®çš„åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è¿™ä¸ªç»éªŒåˆ†å¸ƒè¦æˆä¸ºä¸€ä¸ªå¾ˆå¥½è¿‘ä¼¼çš„çœŸå®åˆ†å¸ƒã€‚
good a model trained on the dataset can get.
è¿™ä¸ªè¿‘ä¼¼çš„è´¨é‡å†³å®šäº†è®­ç»ƒåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šçš„æ¨¡å‹æœ‰å¤šå¥½ã€‚

This is the first constraint on the intelligence of a neural network.
è¿™æ˜¯å…³äºä¸€ä¸ªç¥ç»ç½‘ç»œæ™ºèƒ½ä¸Šçš„ç¬¬ä¸€ä¸ªçº¦æŸã€‚

> [!NOTE]
> æ³¨æ„
>
> **Constraint #1: A model can only be as good as the dataset it was trained on.**
> çº¦æŸ#1ï¼šä¸€ä¸ªæ¨¡å‹åªèƒ½è®­ç»ƒè¾¾åˆ°ä¸å…¶æ‰€ç”¨æ•°æ®é›†ä¸€æ ·çš„è´¨é‡ã€‚
>
> Specifically, the cap on how well a model can approximate the true distribution is determined by how much information about the true distribution is contained within the dataset.
> ç¡®åˆ‡çš„ï¼Œä¸€ä¸ªæ¨¡å‹æœ‰å¤šå¥½çš„å»è¿‘ä¼¼çœŸå®çš„åˆ†å¸ƒï¼Œæ˜¯ç”±åŒ…å«åœ¨å…¶è®­ç»ƒæ•°æ®é›†ä¸­çš„çœŸå®åˆ†å¸ƒçš„ä¿¡æ¯æ•°é‡å†³å®šã€‚

<br />

[^4]: Assuming the true distribution we're trying to model is sufficiently complex to the point where including all information about it in the dataset would be intractable. This is almost always the case in deep learning.
      å‡è®¾æˆ‘ä»¬å°è¯•å»ºæ¨¡çš„çœŸå®çš„åˆ†å¸ƒæ˜¯æœ‰è¶³å¤Ÿçš„å¤æ‚æ€§ï¼Œè€ŒåŒ…å«æ‰€æœ‰è¿™äº›å¤æ‚æ€§åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ˜¯åšä¸åˆ°çš„ã€‚è¿™å‡ ä¹åœ¨æ·±åº¦å­¦ä¹ ä¸­ä¸€ç›´å­˜åœ¨ã€‚
[^5]: Assuming the model perfectly represents all information that exists within the dataset, which rarely happens.
      å‡è®¾æ¨¡å‹å®Œç¾çš„è¡¨å¾äº†æ•°æ®é›†ä¸­çš„å…¨éƒ¨ä¿¡æ¯ï¼Œè€Œè¿™å¾ˆå°‘å‘ç”Ÿã€‚

### A Good Approximation
### ä¸€ä¸ªå¾ˆå¥½çš„è¿‘ä¼¼

To make the empirical distribution a better approximation of the true distribution, we need to include more information about the true distribution in the dataset.
è¦ä½¿å¾—ç»éªŒåˆ†å¸ƒèƒ½æ›´å¥½çš„è¿‘ä¼¼çœŸå®åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦åŒ…å«æ›´å¤šçš„çœŸå®åˆ†å¸ƒçš„ä¿¡æ¯åˆ°è¿™ä¸ªæ•°æ®é›†ä¸­ã€‚

We can increase the total information in the dataset by the information in each individual sample (intuitively, this means using samples that are more informative for the relevant task).
æˆ‘ä»¬èƒ½å¢åŠ æ•°æ®é›†ä¸­çš„æ€»ä½“ä¿¡æ¯ï¼Œå…¶ä¸­çš„æ¯ä¸€ä¸ªç‹¬ç«‹æ ·æœ¬ï¼ˆç›´è§‚ä¸Šï¼Œè¿™æ„å‘³ç€ç”¨åˆ°çš„æ ·æœ¬æ˜¯å¯¹ç›¸å…³ä»»åŠ¡æœ‰æ›´å¤šçš„ä¿¡æ¯é‡ï¼‰

We can also increase the information in the dataset by adding more samples that offer new information about the true distribution[^6].
æˆ‘ä»¬ä¹Ÿèƒ½å¢åŠ æ•°æ®çš„ä¿¡æ¯ï¼Œé€šè¿‡åŠ å…¥æ›´å¤šçš„æ ·æœ¬ï¼Œå…±åŒå…³äºçœŸå®åˆ†å¸ƒçš„æ–°ä¿¡æ¯ã€‚

**To simplify, there are two ways to improve the quality of the dataset:**
ç®€å•çš„è¯´ï¼Œæœ‰ä¸¤ä¸ªæ–¹æ³•å»æå‡æ•°æ®é›†çš„è´¨é‡ï¼š

1. data quality
   æ•°æ®å®šæ€§
3. data quantity
   æ•°æ®å®šé‡

This is not because more data is always good[^7], but because we want more information about the true distribution in the dataset so the model can learn a sufficient approximation of it.
è¿™ä¸æ˜¯å› ä¸ºæ›´å¤šçš„æ•°æ®æ˜¯ä¸€ç›´å¾ˆå¥½çš„ï¼Œè€Œæ˜¯å› ä¸ºæˆ‘ä»¬æƒ³è¦æ›´å¤šå…³äºçœŸå®åˆ†å¸ƒçš„ä¿¡æ¯å‡ºç°åœ¨æ•°æ®é›†ä¸­ï¼Œè¿™æ ·æ¨¡å‹èƒ½å­¦ä¹ åˆ°ä¸€ä¸ªè¶³å¤Ÿçš„è¿‘ä¼¼ç»“æœã€‚

With this understanding of the data constraint and how to improve the quality of datasets, we can look at how progress in this dimension has impacted the history of deep learning.
æœ‰äº†è¿™ä¸ªå…³äºæ•°æ®é›†çº¦æŸå’Œå¦‚ä½•æå‡æ•°æ®é›†è´¨é‡çš„ç†è§£ï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°åœ¨è¿™ä¸ªç»´åº¦ä¸Šçš„è¿›åº¦å¦‚ä½•èƒ½å½±å“æ·±åº¦å­¦ä¹ çš„å†å²è¿›ç¨‹ã€‚

<br />

[^6]: This is analogous to how adding more terms to a Taylor series yields a function closer to the original. Approximations improve with more information about the true function.
      è¿™æ˜¯æ¨¡æ‹Ÿäº†å¦‚ä½•åŠ å…¥æ›´å¤šçš„é¡¹ç»™Taylorçº§æ•°ï¼Œå¾—åˆ°ä¸€ä¸ªæ›´æ¥è¿‘åŸå§‹çš„å‡½æ•°ã€‚è¿‘ä¼¼æå‡æ˜¯é€šè¿‡æ›´å¤šçš„å…³äºçœŸå®å‡½æ•°çš„ä¿¡æ¯åšåˆ°ã€‚
[^7]: In fact, you can think of examples where more data makes no difference. For example adding the same image to a dataset (or two images similar to each other) doesn't improve the quality of the model created. It's because these new 
      äº‹å®ä¸Šï¼Œæˆ‘ä»¬èƒ½æƒ³åˆ°æ ·æœ¬ä¸­çš„å¤§éƒ¨åˆ†æ•°æ®æ²¡æœ‰å·®å¼‚ã€‚ä¾‹å¦‚ï¼ŒåŠ å…¥ç›¸åŒçš„å›¾ç‰‡åˆ°ä¸€ä¸ªæ•°æ®é›†æ±‡æ€»ï¼ˆæˆ–ä¸¤å¼ ä¸€æ ·çš„å›¾ç‰‡ï¼‰ä¸èƒ½æå‡åˆ›å»ºæ¨¡å‹çš„è´¨é‡ã€‚è¿™æ˜¯å› ä¸ºè¿™äº›æ–°çš„æ•°æ®ç‚¹ä¸èƒ½åŠ å…¥æ›´å¤šçš„å…³äºçœŸå®åˆ†å¸ƒçš„æ–°ä¿¡æ¯ã€‚
data points don't add much new information about the true distribution.

### Breakthrough #1: Large Labeled Datasets
### çªç ´ #1ï¼šå¤§å‹çš„æ ‡è®°æ•°æ®é›†

Early machine learning relied on datasets collected by individual research teams. Despite the development of effective approaches to deep learning, datasets weren't large enough to prove their advantages.
æ—©æœŸçš„æœºå™¨å­¦ä¹ ä¾èµ–äºæ•°æ®é›†éƒ½æ˜¯ç”±ç‹¬ç«‹çš„ç ”ç©¶å›¢é˜Ÿæ”¶é›†ã€‚å°½ç®¡æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆå‘å±•ï¼Œä½†æ˜¯æ•°æ®é›†ä¸å¤Ÿå¤§ï¼Œå¯¼è‡´æ— æ³•è¯æ˜å…¶ä¼˜ç‚¹ã€‚

The introduction of datasets like [MNIST](https://en.wikipedia.org/wiki/MNIST_database) and [ImageNet](https://en.wikipedia.org/wiki/ImageNet) drastically increased the availability of high quality datasets large enough to effectively 
éšç€ä¸€äº›æ•°æ®é›†çš„å¼•å…¥ï¼Œå¦‚MNISTå’ŒImagNetï¼Œæ˜¾è‘—çš„æå‡äº†é«˜è´¨é‡æ•°æ®é›†çš„å¯ç”¨æ€§ï¼Œå…¶è¶³å¤Ÿå¤§ï¼Œå¯ä»¥ç”¨æ¥è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
train deep learning models.

Early [CNNs](/01-deep-neural-networks/02-cnn/03-cnn.ipynb) like [LeNet](/01-deep-neural-networks/02-cnn/02-le-net.pdf) and [AlexNet](/01-deep-neural-networks/03-alex-net/01-alex-net.pdf) used these datasets to show that deep neural 
æ—©æœŸçš„CNNï¼Œç±»ä¼¼LeNetå’ŒAlexNetç”¨åˆ°äº†è¿™äº›æ•°æ®é›†è¯æ˜äº†
networks could compete with the traditional machine learning approaches used at the time.
æ·±åº¦ç¥ç»ç½‘ç»œèƒ½ä¸åŒæœŸçš„ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•ç«äº‰ã€‚

It's easy to take for granted the impact of these datasets now, as they have long been obsolete - but they clearly had a huge impact on the field. Notably, AlexNet, which [completely changed the field of deep learning](/01-deep-neural-
ç°åœ¨å¾ˆå®¹æ˜“ç¡®è®¤è¿™äº›æ•°æ®é›†çš„å½±å“åŠ›ï¼Œå› ä¸ºè¿™äº›éƒ½å·²ç»æ˜¯è¿‡æ—¶çš„ï¼Œä½†æ˜¯è¿™äº›éƒ½å¯¹é¢†åŸŸæœ‰å·¨å¤§çš„å½±å“ã€‚ç‰¹åˆ«æ˜¯ï¼ŒAlexNetï¼Œå…¶å®Œå…¨æ”¹å˜äº†æ·±åº¦å­¦ä¹ çš„é¢†åŸŸï¼Œè€ŒImageNetæ•°æ®é›†å¯¹å…¶è€Œè¨€æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚
networks/03-alex-net/01-alex-net.pdf), could not have existed without the creation of the ImageNet dataset.

**The introduction of large labeled datasets can be seen as the first breakthrough in pushing the data constraint toward larger datasets.**
å¼•å…¥äº†å¤§å‹çš„æ ‡è®°æ•°æ®é›†èƒ½è¢«çœ‹ä½œç¬¬ä¸€ä¸ªçªç ´ï¼Œæ¨åŠ¨äº†æ•°æ®çº¦æŸå‘ç€æ›´å¤§çš„æ•°æ®é›†å‘å±•ã€‚

Though useful, these datasets were inherently unscalable due to the manual labeling process they rely on. In order to push the data constraint to the next level with even larger datasets, a new approach to data was needed.
å°½ç®¡æœ‰ç”¨ï¼Œè¿™äº›æ•°æ®é›†å®é™…æ˜¯æ— æ³•æ‰©å±•çš„ï¼Œå› ä¸ºéƒ½ä¾èµ–äºäººå·¥æ ‡è®°ã€‚ä¸ºäº†æ¨åŠ¨æ•°æ®çº¦æŸæœç€ä¸‹ä¸€ä¸ªå±‚çº§æ›´å¤§æ•°æ®é›†å‘å±•ï¼Œå°±éœ€è¦ä¸€ä¸ªæ–°çš„æ•°æ®æ–¹æ³•ã€‚

<br />

### Breakthrough #2: Unlocking the Internet
çªç ´2ï¼šè§£é”äº’è”ç½‘

The internet is the most obvious source of massive amounts of data that could plausibly be used for deep learning. However, it was initially unclear how to use this data to train a deep learning model.
äº’è”ç½‘æ˜¾ç„¶æ˜¯æµ·é‡çš„æ•°æ®é›†æºï¼Œèƒ½åˆç†ç”¨äºæ·±åº¦å­¦ä¹ ã€‚ä½†æ˜¯ï¼Œä¸€å¼€å§‹ä¸æ¸…æ¥šå¦‚ä½•ç”¨è¿™äº›æ•°æ®å»è®­ç»ƒä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

Unlike labeled datasets, internet data is not created for a specific tasks, so it didn't appear to contain high quality data that could contribute to training a specific model. For this reason, internet data appeared to be unusable in 
ä¸åŒäºå·²æ ‡è®°çš„æ•°æ®ï¼Œäº’è”ç½‘æ•°æ®ä¸æ˜¯ç”¨äºç‰¹å®šçš„ä»»åŠ¡çš„ï¼Œè¿™æ ·å…¶å°±ä¸èƒ½åŒ…å«é«˜è´¨é‡çš„æ•°æ®ï¼Œæ— æ³•ç”¨æ¥è®­ç»ƒä¸€ä¸ªç‰¹å®šçš„æ¨¡å‹ã€‚å› æ­¤ï¼Œäº’è”ç½‘æ•°æ®åœ¨æ·±åº¦å­¦ä¹ çš„å‘å±•ä¸­æœ‰å¾ˆé•¿ä¸€æ®µæ—¶é—´æ˜¯æ— ç”¨çš„ã€‚
deep learning for a long time[^8].

[BERT](/04-transformers/02-bert/03-bert.ipynb) completely changed this. BERT popularized the **transfer learning** paradigm now used by all large language models (including [GPTs](/04-transformers/04-gpt/03-gpt.ipynb)) - the model was 
BERTå®Œå…¨æ”¹å˜äº†è¿™ä¸€åˆ‡ã€‚BERTä¸»å¯¼äº†è¿ç§»å­¦ä¹ çš„æ–¹æ³•ï¼Œç°åœ¨è¢«ç”¨åˆ°äº†æ‰€æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼ˆåŒ…æ‹¬GPTï¼‰-
_pre-trained_ on a large portion of the internet (high quantity, unpredictable quality), and then _fine-tuned_ on smaller datasets (low quantity, high quality).
è¿™ä¸ªæ¨¡å‹æ˜¯é¢„è®­ç»ƒåœ¨ä¸€ä¸ªå¤§å‹çš„äº’è”ç½‘æ•°æ®é›†ä¸Šï¼ˆæµ·é‡ï¼Œæ— æ³•é¢„æµ‹çš„è´¨é‡ï¼‰ï¼Œå¹¶å¾®è°ƒåœ¨æ›´å°çš„æ•°æ®é›†ä¸Šï¼ˆå¾ˆå°‘æ•°é‡ï¼Œé«˜è´¨é‡ï¼‰ã€‚

**For the first time ever, BERT showed that we could actually make internet-scale datasets useful.**
ç¬¬ä¸€æ¬¡ï¼ŒBERTè¯æ˜äº†æˆ‘ä»¬èƒ½è®©äº’è”ç½‘ä¸Šçš„æ•°æ®é›†éå¸¸æœ‰ç”¨ã€‚

The results also shocked the broader tech community - for example, [causing a Google executive to express](https://x.com/TechEmails/status/1756765277478621620) that an AI system would inevitably replace Google search in the near future.
è¿™ä¸ªç»“æœä¹ŸæŒ¯åŠ¨äº†æ›´å¤§æŠ€æœ¯ç¤¾åŒº-ä¾‹å¦‚ï¼Œå¯¼è‡´Goolgeå‘å¸ƒäº†ä¸€ä¸ªå£°æ˜ï¼ŒAIç³»ç»Ÿå°†ä¸å¯é¿å…çš„æ›¿ä»£Googleæœç´¢ï¼Œå°±åœ¨ä¸è¿œçš„å°†æ¥ã€‚

For those curious, the [LoRA](/04-transformers/05-lora/02-lora.ipynb) paper further developed on why the transfer learning paradigm developed by BERT and used by all modern LLMs may be so effective.
ä¸ºæ­¤ï¼ŒLoRAè®ºæ–‡è¿›ä¸€æ­¥ç ”ç©¶å‘å¸ƒäº†ï¼Œä¸ºä»€ä¹ˆBERTæå‡ºçš„è¿ç§»å­¦ä¹ æ–¹æ³•å¹¶è¢«æ‰€æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨ï¼Œæ˜¯å¦‚æ­¤çš„æœ‰æ•ˆã€‚

<br />

[^8]: There was not powerful enough compute or good enough architectures to process the scale of internet datasets effectively for a long time.
      å¾ˆé•¿ä¸€æ®µæ—¶é—´ï¼Œæ²¡æœ‰è¶³å¤Ÿå¼ºå¤§çš„è®¡ç®—ï¼Œæˆ–è¶³å¤Ÿå¥½çš„æ¶æ„å»æœ‰æ•ˆçš„å¤„ç†äº’è”ç½‘è§„æ¨¡çš„æ•°æ®é›†ã€‚

### Breakthrough #3: Training Assistants
çªç ´3ï¼šè®­ç»ƒè¾…åŠ©

[BERT](/04-transformers/02-bert/03-bert.ipynb) and the [GPTs](/04-transformers/04-gpt/03-gpt.ipynb) were technically impressive but didn't immediately reach the mainstream until the release of ChatGPT.
BERTå’ŒGPTsæŠ€æœ¯ä¸Šå¾ˆå‡ºè‰²ï¼Œä½†æ˜¯æ²¡æœ‰ç«‹åˆ»æˆä¸ºä¸»æµï¼ŒçŸ¥é“ChatGPTçš„å‘å¸ƒã€‚

[InstructGPT](/04-transformers/06-rlhf/05-rlhf.ipynb) was the breakthrough that enabled this. It used [RLHF](/04-transformers/06-rlhf/) techniques to fine-tune the base GPT-3 model using a human generated dataset of question-answer 
InstructGPTæ˜¯èƒ½åšåˆ°è¿™äº›çš„çªç ´ç‚¹ã€‚å…¶ç”¨åˆ°äº†ä¸€äº›æŠ€æœ¯å»å¾®è°ƒåŸºç¡€çš„GPT-3æ¨¡å‹ï¼Œç”¨åˆ°äº†ä¸€ä¸ªäººå·¥ç”Ÿæˆçš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬äº†æˆå¯¹çš„é—®ç­”ï¼Œä½œä¸ºä¸€ä¸ªéå¸¸æœ‰ç”¨çš„åŠ©æ‰‹å»æä¾›å¾ˆå¥½çš„åé¦ˆã€‚
pairs deemed good responses for a helpful assistant.

By learning to behave effectively as an assistant, InstructGPT created the practical communication style that enabled ChatGPT to succeed.
é€šè¿‡å­¦ä¹ å»æœ‰æ•ˆæˆä¸ºä¸€ä¸ªåŠ©æ‰‹ï¼ŒInstructGPTåˆ›å»ºäº†å®ç”¨çš„äº¤æµé£æ ¼ï¼Œä½¿å¾—ChatGPTæˆåŠŸã€‚

**The success of InstructGPT is an indication of how high-leverage data quality can be when fine-tuning language-models.**
InstructGPTçš„æˆåŠŸè¯æ˜äº†ï¼Œåœ¨å¾®è°ƒè¯­è¨€æ¨¡å‹å¦‚ä½•å»é«˜åº¦çš„åˆ©ç”¨æ•°æ®é›†è´¨é‡ã€‚

Though many fine-tuned models existed before the instruct series, InstructGPT was far preferred to almost everything else at the time due to the high quality data it was trained on.
å°½ç®¡å¤§é‡å¾®è°ƒåçš„æ¨¡å‹åœ¨æŒ‡ä»¤ç³»åˆ—ä¹‹å‰å°±å‡ºç°äº†ï¼ŒInstructGPTå‡ ä¹æ˜¯é‚£æ—¶å…¶ä¸­æœ€å¥½çš„ï¼Œè¿™æ˜¯å› ä¸ºå…¶è®­ç»ƒåœ¨é«˜è´¨é‡çš„æ•°æ®ä¸Šã€‚

<br />

### Beyond Internet Data
è¶…è¶Šäº’è”ç½‘æ•°æ®

How much more can we improve the quality of the datasets deep learning models are trained on to improve the capacity for models to become intelligent?
æˆ‘ä»¬ç©¶ç«Ÿè¦æå‡åˆ°æ€æ ·çš„æ•°æ®é›†è´¨é‡ï¼Œä½¿å¾—æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å…¶ä¸Šè®­ç»ƒï¼Œèƒ½æå‡æ¨¡å‹çš„èƒ½åŠ›ï¼Œè®©å…¶å˜å¾—æ™ºèƒ½ï¼Ÿ

The amount of data generated on the internet is increasing exponentially, which should continue to provide a source of increasingly large datasets to train on[^9].
äº’è”ç½‘ä¸Šç”Ÿæˆæ•°æ®çš„æ•°é‡æ˜¯æŒ‡æ•°å¢é•¿çš„ï¼Œè¿™æ ·å°±èƒ½æŒç»­æä¾›ä¸€ä¸ªä¸æ–­å¢é•¿çš„å¤§å‹æ•°æ®é›†å¯ä»¥ç”¨ä½œè®­ç»ƒã€‚

However, there's another question about the quality of the data on internet-scale datasets. We want our systems to model reality - whereas the internet can be understood as a (highly) lossy compression of the true laws of reality[^10].
ä½†æ˜¯ï¼Œæœ‰å¦ä¸€ä¸ªé—®é¢˜å…³äºäº’è”ç½‘è§„æ¨¡çš„æ•°æ®é›†çš„è´¨é‡çš„ã€‚å› ä¸ºæˆ‘ä»¬æƒ³è¦çš„ç³»ç»Ÿæ˜¯å»å»ºæ¨¡çœŸå®ä¸–ç•Œ-ç”±äºäº’è”ç½‘èƒ½è¢«ç†è§£æˆä¸ºä¸€ä¸ªï¼ˆé«˜åº¦ï¼‰æœ‰æŸå‹ç¼©çš„çœŸå®ä¸–ç•Œæ³•åˆ™ã€‚

Because of this, the abundance of humanoid robots may present a new means of data collection for deep learning models that gives direct access to information about reality - which makes [OpenAI & Microsoft's investment and collaboration 
å› æ­¤ï¼Œä¸°å¯Œçš„ç±»äººæœºå™¨äººå¯èƒ½å±•ç¤ºäº†ä¸€ä¸ªæ–°çš„æ„ä¹‰ï¼Œå³æ•°æ®æ”¶é›†ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½ç›´æ¥ç”¨åˆ°å…³äºçœŸå®ä¸–ç•Œçš„ä¿¡æ¯-è¿™å˜å¾—ç‰¹åˆ«æœ‰è¶£ã€‚

with Figure](https://www.reuters.com/technology/robotics-startup-figure-raises-675-mln-microsoft-nvidia-other-big-techs-2024-02-29/) particularly interesting.

Regardless, current scaling laws have shown that current models are far from reaching the capacity of the information available in internet-scale datasets, meaning we may be far away from the point where data becomes the constraint 
æ¯«æ— ç–‘é—®ï¼Œå½“å‰çš„å°ºåº¦æ³•åˆ™å·²ç»è¯æ˜äº†ï¼Œå½“å‰çš„æ¨¡å‹æ˜¯è¿œè¿œæ²¡åˆ°å®¹çº³äº’è”ç½‘æ•°æ®é›†çš„å¯ç”¨ä¿¡æ¯çš„è§„æ¨¡ï¼Œä¹Ÿæ„å‘³ç€æˆ‘ä»¬è¿˜è¿œæ²¡åˆ°æ•°æ®æˆä¸ºçº¦æŸçš„ç‚¹ã€‚
again.

<br />

[^9]: This may not actually be sufficient to keep increasing the quality of models, as a recent [analysis of zero-shot learning](https://arxiv.org/abs/2404.04125) shows that large language models ability to perform tasks increases 
      è¿™å¯èƒ½å®é™…ä¸è¶³ä»¥ä¿è¯æ¨¡å‹çš„è´¨é‡å¢é•¿ï¼Œå¦‚æœ€è¿‘çš„analysis of zero-shotæ‰€è®ºè¿°çš„ï¼Œå¤§è¯­è¨€æ¨¡å‹æœ‰èƒ½åŠ›å®ç°æ‰§è¡Œä»»åŠ¡çš„å¯¹æ•°å¢é•¿ï¼Œç›¸è¾ƒäºæ•°æ®é›†ä¸­çš„ç›¸å…³æ•°æ®é‡è€Œè¨€ã€‚
logartihmically with the amount of relevant data in the dataset.
[^10]: The internet is a lossy compression of the entirety of human knowledge, with lot's of noise (complex and contrasting intentions behind different posts). Additionally, human knowledge itself is a very lossy (and partially 
        äº’è”ç½‘æ˜¯ä¸€ä¸ªæœ‰æŸå‹ç¼©çš„äººç±»çŸ¥è¯†çš„å…¨é›†ï¼Œæœ‰å¤§é‡çš„å™ªå£°ï¼ˆå¤æ‚æ€§ï¼Œå’Œä¸åŒå‘å¸ƒè€…çš„å¸¦æ¥çš„ä¸åŒæ„å›¾ï¼‰ã€‚æ­¤å¤–ï¼Œäººç±»çŸ¥è¯†æœ¬èº«æ˜¯ä¸€ä¸ªéå¸¸æœ‰æŸå¤±ï¼ˆåŒæ—¶éƒ¨åˆ†ä¸å‡†ç¡®ï¼‰å‹ç¼©çš„çœŸå®ä¸–ç•Œæ³•åˆ™ã€‚
inaccurate) compression of the laws of reality.

### Modeling Data
å»ºæ¨¡æ•°æ®

Now that we've understood the data constraint, we can explore what constrains how effectively the neural network can model the data.
ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»ç†è§£äº†æ•°æ®çº¦æŸï¼Œæˆ‘ä»¬å°±èƒ½æ¢ç´¢å…³äºç¥ç»ç½‘ç»œå¦‚ä½•èƒ½æœ‰æ•ˆå»ºæ¨¡æ•°æ®çš„çº¦æŸä¸Šã€‚

This determines how close to modeling the empirical distribution the model will get, which corresponds with its intelligence.
è¿™å†³å®šäº†å»ºæ¨¡ç»éªŒåˆ†å¸ƒçš„æ¨¡å‹å°†æœ‰å¤šæ¥è¿‘çœŸå®åˆ†å¸ƒï¼Œè¿™å…³ç³»åˆ°å…¶æ™ºèƒ½æ€§ã€‚

The first constraint that determines the capacity for the model to learn the empirical distribution is the number of parameters in the neural network.
ç¬¬ä¸€ä¸ªçº¦æŸå…³äºæ¨¡å‹å­¦ä¹ ç»éªŒåˆ†å¸ƒçš„å®¹é‡ï¼Œå°±æ˜¯ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°æ•°é‡ã€‚

<br />

## 1.2. Parameters
        å‚æ•°

![constraint-2-parameters](./images/readme/constraint-2-parameters.png)

The model needs to have enough _representational capacity_ to be able to learn the empirical distribution of the dataset.
æ¨¡å‹éœ€è¦æœ‰è¶³å¤Ÿçš„è¡¨å¾èƒ½åŠ›å»å­¦ä¹ æ•°æ®é›†ä¸­çš„ç»éªŒåˆ†å¸ƒã€‚

This means the neural network needs to have parameters to provide enough degrees of freedom to accurately model the distribution. In practice, it's challenging to predict the minimal number of parameters needed to fully model a dataset.
è¿™æ„å‘³ç€ç¥ç»ç½‘ç»œéœ€è¦æœ‰å‚æ•°å¯ä»¥æä¾›è¶³å¤Ÿçš„è‡ªç”±åº¦å»å‡†ç¡®çš„å»ºæ¨¡åˆ†å¸ƒã€‚å®é™…ä¸Šï¼Œé¢„æµ‹å…¨é¢å»ºæ¨¡ä¸€ä¸ªæ•°æ®é›†æ‰€éœ€çš„æœ€å°å‚æ•°é‡æ˜¯æœ‰æŒ‘æˆ˜æ€§çš„äº‹æƒ…ã€‚


However, when the amount of information in the dataset is far beyond what the network is capable of modeling, the easiest way to improve the network is to scale up the number of parameters - which can mean increasing the depth of the 
ä½†æ˜¯ï¼Œå½“æ•°æ®é›†ä¸­çš„ä¿¡æ¯é‡è¿œè¿œè¶…è¿‡äº†å»ºæ¨¡èƒ½åŠ›ï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯å»æå‡ç½‘ç»œï¼Œé€šè¿‡å¢åŠ å‚æ•°æ•°é‡çš„æ–¹å¼-è¿™èƒ½å‡åŒ€çš„æå‡ç½‘ç»œçš„æ·±åº¦ï¼Œ
network and adding more parameters per layer.
å¹¶åœ¨æ¯ä¸€å±‚åŠ å…¥æ›´å¤šçš„å‚æ•°ã€‚

With modern internet-scale datasets, the complexity is massive, so the approach of adding more parameters shows no signs of slowing down in terms of its efficacy at improving the intelligence of models.
ç”¨äº†ç°ä»£äº’è”ç½‘è§„æ¨¡çš„æ•°æ®é›†ï¼Œå°±æœ‰äº†æå¤§çš„å¤æ‚æ€§ï¼Œè¿™æ ·åŠ å…¥æ›´å¤šå‚æ•°çš„æ–¹æ³•ï¼Œè¿˜æ²¡æœ‰å‡ºç°åœ¨æå‡æ¨¡å‹æ™ºèƒ½æ€§ä¸Šæœ‰æ”¾ç¼“çš„è¿¹è±¡ã€‚

> [!NOTE]
>æ³¨æ„
> 
> **Constraint #2: The representational capacity of a model is bounded by the number of parameters it contains.**
> çº¦æŸ #2ï¼šä¸€ä¸ªæ¨¡å‹çš„è¡¨å¾èƒ½åŠ›æ˜¯å—åˆ°å…¶åŒ…å«çš„å‚æ•°é‡çš„åˆ¶çº¦ã€‚

In practice, we'll see that increasing the number of parameters in a neural network is actually a function of the other constraints.
å®é™…ä¸Šï¼Œæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œå¢åŠ ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°é‡æ˜¯ä¸å…¶å®ƒçº¦æŸæ¡ä»¶æœ‰å‡½æ•°å…³ç³»ã€‚

Let's look at the times in the past where this constraint has been particularly relevant.
è®©æˆ‘ä»¬çœ‹çœ‹è¿‡å»å‘ç”Ÿçš„ç‰¹åˆ«ç›¸å…³çš„ä¸€äº›çº¦æŸã€‚

<br />

### Breakthrough #1: Increasing Depth
### çªç ´ #1ï¼šå¢åŠ æ·±åº¦

The earliest neural networks consisted of just a single input and output layer, heavily limiting their representational capacity.
æœ€æ—©çš„ç¥ç»ç½‘ç»œåªæœ‰ä¸€ä¸ªå•ä¸€çš„è¾“å…¥å’Œè¾“å‡ºå±‚ï¼Œä¸¥é‡åˆ¶çº¦äº†å…¶è¡¨å¾èƒ½åŠ›ã€‚

The original [backpropagation paper](/01-deep-neural-networks/01-dnn/01-dnn.pdf) discussed the addition of a hidden layer, adding more parameters to the network which significantly increased it's ability to represent more complex 
æœ€åˆçš„åå‘ä¼ æ’­è®ºæ–‡è®¨è®ºäº†åŠ å…¥ä¸€ä¸ªéšè—å±‚ï¼ŒåŠ å…¥æ›´å¤šçš„å‚æ•°åˆ°ç½‘ç»œä¸­ï¼Œèƒ½æ˜¾è‘—çš„æå‡å…¶è¡¨å¾æ›´å¤æ‚æ€§çš„èƒ½åŠ›ã€‚

problems (like shift-registers, the XOR gate, etc. - all very simple examples, but impressive at the time).
ä¸€äº›é—®é¢˜ï¼ˆç±»ä¼¼ç§»åŠ¨-å¯„å­˜å™¨ï¼ŒXORé—¨æ§ï¼Œç­‰ã€‚æ‰€æœ‰è¿™äº›éƒ½æ˜¯éå¸¸ç®€å•çš„ä¾‹å­ï¼Œä½†æ˜¯åœ¨é‚£æ—¶è®©äººå°è±¡æ·±åˆ»ï¼‰ã€‚


[AlexNet](/01-deep-neural-networks/03-alex-net/02-alex-net.ipynb) is one of the clearest examples of increasing parameters leading to better models[^11] - the AlexNet architecture used 5 convolutional layers, far more than the previous 
AlexNetæ˜¯å…¶ä¸­æœ€æ¸…æ¥šçš„ä¾‹å­ï¼Œå…¶å¢åŠ äº†å‚æ•°å¾—åˆ°äº†æ›´å¥½çš„æ¨¡å‹-AlexNetæ¶æ„ä¸­ç”¨åˆ°äº†5ä¸ªå·ç§¯å±‚ï¼Œè¿œè¿œè¶…è¿‡äº†å‰ä¸€ä¸ª

largest CNN at the time, which enabled it to crush the previous best score in the ImageNet competition.
å½“æ—¶æœ€å¤§çš„CNNï¼Œè¿™æ ·ä½¿å¾—å…¶èƒ½å‹å€’ä¹‹å‰æœ€å¥½çš„æˆç»©åœ¨ImageNetç«èµ›ä¸Šã€‚

However, early on, size appeared to be one of many factors constraining the improvement of models, rather than the most important constraint.
ä½†æ˜¯ï¼Œæ—©å‰ï¼Œå¤§å°æ˜¯çº¦æŸæ¨¡å‹æå‡çš„ä¸€ä¸ªä¸»è¦å› ç´ ï¼Œè¶…è¿‡äº†å…¶å®ƒçš„å› ç´ ã€‚


<br />

[^11]: Although, AlexNet was the result of a large number of innovations that combined to make it so effective - the increase in network depth was complemented with a use of effective optimization & regularization methods and the use of 
å°½ç®¡AlexNetæ˜¯å¤§é‡åˆ›æ–°çš„ç»“æœï¼Œè¿™äº›åˆ›æ–°çš„ç»„åˆä½¿å¾—å…¶å¦‚æ­¤æœ‰æ•ˆ-åœ¨ç½‘ç»œæ·±åº¦çš„æå‡ï¼Œç»„åˆç”¨åˆ°äº†ä¼˜åŒ–å’Œæ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œå¹¶ç”¨GPUè®­ç»ƒï¼Œ
GPUs for training which enabled this increase in size.
è¿™æ ·ä½¿å¾—å¤§å°å¾—åˆ°äº†æå‡ã€‚

### Breakthrough #2: Scaling Laws
### çªç ´ #2ï¼šç¼©æ”¾æ³•åˆ™

The [GPT](/04-transformers/04-gpt/) series made it clear that for internet datasets, scaling parameters appears to be sufficient to significantly increase model quality.
GPTç³»åˆ—è¯æ˜ï¼Œå¯¹äºäº’è”ç½‘æ•°æ®é›†ï¼Œç¼©æ”¾å‚æ•°çš„åšæ³•æ˜¯è¶³ä»¥æ˜¾è‘—çš„æå‡æ¨¡å‹çš„è´¨é‡ã€‚

The scaling laws show no sign of letting up, which has motivated the current continued attempts at training larger and larger models.
ç¼©æ”¾æ³•åˆ™è¿˜æ²¡æœ‰æ˜¾ç¤ºå‡ºåœæ­¢çš„è¿¹è±¡ï¼Œè¿™ä¿ƒä½¿äº†å½“å‰ç»§ç»­å°è¯•è®­ç»ƒè¶Šæ¥è¶Šå¤§çš„æ¨¡å‹ã€‚


<p align="center">
  <img src="/images/readme/scaling-laws.png" alt="Scaling Laws" width="50%" />
</p>
<p align="center">
  <i>Scaling laws for model performance as a function of parameters</i>
  ç¼©æ”¾æ³•åˆ™å¯¹äºæ¨¡å‹æ€§èƒ½è€Œè¨€ï¼Œç±»ä¼¼ä¸€ä¸ªå‚æ•°åŒ–çš„å‡½æ•°ã€‚
</p>

**Importantly, the reason for this trend is not that increasing the number of parameters in a model always increases it's intelligence.** Instead, it's due to the fact that current models still don't have enough representational 
é‡è¦çš„æ˜¯ï¼Œå‡ºç°è¿™ä¸ªè¶‹åŠ¿çš„åŸå› å¹¶ä¸æ˜¯å¢åŠ æ¨¡å‹çš„å‚æ•°é‡èƒ½ä¸€ç›´æå‡å…¶æ™ºèƒ½ã€‚è€Œæ˜¯ç”±äºäº‹å®ä¸Šå½“å‰çš„

capacity to capture all the information in internet-scale datasets.
æ¨¡å‹å®¹é‡ä»ç„¶ä¸è¶³ä»¥å»è¡¨å¾äº’è”ç½‘è§„æ¨¡æ•°æ®é›†çš„å…¨éƒ¨ä¿¡æ¯ã€‚


As mentioned previosly, increasing the number of parameters in a neural network is actually governed by the other constraints.
å¦‚ä¹‹å‰æ‰€æåˆ°çš„ï¼Œå¢åŠ ç¥ç»ç½‘ç»œçš„å‚æ•°é‡å®é™…ä¸Šæ˜¯å—åˆ°å…¶å®ƒæ¡ä»¶çš„åˆ¶çº¦ã€‚

<br />

## 1.3. Optimization & Regularization
## 1.3. ä¼˜åŒ–å’Œæ­£åˆ™åŒ–

![constraint-3-optimization-and-regularization](./images/readme/constraint-3-optimization-and-regularization.png)
çº¦æŸ-3-ä¼˜åŒ–å’Œæ­£åˆ™åŒ–

In reality, you can't keep scaling up the number of parameters in a model and expect quality to keep increasing.Scaling up a model (via increasing the depth or the number of parameters per layer) introduces two new classes of problems.
ç°å®ä¸­ï¼Œä½ ä¸å¯èƒ½æŒç»­æ”¾å¤§æ¨¡å‹çš„å‚æ•°é‡ï¼Œå¹¶æœŸæœ›å…¶è´¨é‡ä¸€ç›´ä¸Šå‡ã€‚æ”¾å¤§ä¸€ä¸ªæ¨¡å‹ï¼ˆé€šè¿‡å¢åŠ æ·±åº¦æˆ–æ¯ä¸€å±‚çš„å‚æ•°é‡ï¼‰å¼•å…¥äº†ä¸¤ç§æ–°ç±»å‹çš„é—®é¢˜ã€‚

First, increasing the depth of a network can make it take far longer to converge to an optimal solution, or in the worst cases, can prevent the network from converging.
é¦–å…ˆï¼Œå¢åŠ ä¸€ä¸ªç½‘ç»œçš„æ·±åº¦ï¼Œå°†ä½¿å¾—æ›´éš¾çš„æ”¶æ•›åˆ°ä¸€ä¸ªæœ€ä¼˜è§£ï¼Œæˆ–æœ€åœ¨æœ€åçš„æƒ…å†µï¼Œå¯èƒ½å¯¼è‡´ç½‘ç»œä¸èƒ½æ”¶æ•›ã€‚

**The process of ensuring models can converge effectively, even as they grow in depth, is known as optimization.**
ç¡®ä¿æ¨¡å‹èƒ½æœ‰æ•ˆæ”¶æ•›çš„è¿‡ç¨‹ï¼Œç”šè‡³åœ¨å¢åŠ æ·±åº¦æ¡ä»¶ä¸‹ï¼Œè¢«ç§°ä¸ºä¼˜åŒ–ã€‚


Additionally, when you scale up the number of parameters in a model so it's representational capacity exceeds the complexity of the empirical distribution, the model can start fitting trivial _noise_ in the distribution. This effect is 
æ­¤å¤–ï¼Œå½“ä½ æ”¾å¤§æ¨¡å‹çš„å‚æ•°é‡ï¼Œè¿™æ ·å…¶è¡¨å¾èƒ½åŠ›è¶…è¿‡äº†ç»éªŒåˆ†å¸ƒçš„å¤æ‚æ€§ï¼Œè¿™ä¸ªæ¨¡å‹å°±å¼€å§‹æ‹Ÿåˆåˆ†å¸ƒä¸­çš„å¾ˆå°çš„å™ªå£°ï¼Œ

known as _overfitting_.
è¿™ä¸ªç°è±¡è¢«ç§°ä¸ºè¿‡æ‹Ÿåˆã€‚

**The process of regularization is used to ensure models learn useful _generalizations_ of the dataset and don't overfit to noise.**
æ­£åˆ™åŒ–çš„è¿‡ç¨‹è¢«ç”¨æ¥ç¡®ä¿æ¨¡å‹å­¦ä¹ æ•°æ®é›†ä¸­æœ‰ç”¨çš„æ³›åŒ–æ€§ï¼Œå¹¶ä¸å‡ºç°è¿‡æ‹Ÿåˆå™ªå£°çš„æƒ…å†µã€‚

In practice, the actual depth of a network is constrained by the efficacy of the optimization & regularization strategies used.
å®é™…ä¸Šï¼Œä¸€ä¸ªç½‘è·¯çš„å®é™…æ·±åº¦æ˜¯å—åˆ°çº¦æŸçš„ï¼Œå³å—æ‰€ç”¨åˆ°çš„ä¼˜åŒ–å’Œæ­£åˆ™åŒ–ç­–ç•¥çš„æ•ˆç‡çº¦æŸã€‚

> [!NOTE]
> æ³¨æ„
> 
> **Constraint #3: The efficacy of optimization & regularization approaches constrains the number of parameters a network can handle while still being able to converge and generalize.**
> çº¦æŸ3ï¼šä¼˜åŒ–å’Œæ­£åˆ™åŒ–æ–¹æ³•çš„æ•ˆç‡çº¦æŸä¸€ä¸ªç½‘ç»œçš„å‚æ•°é‡èƒ½è¢«å¤„ç†ï¼Œå¹¶ä»ç„¶èƒ½ä¿è¯æ”¶æ•›å’Œæ³›åŒ–ã€‚

<br />

### Breakthrough #1: Taming Gradients
### çªç ´ #1ï¼šæ§åˆ¶æ¢¯åº¦

While training deeper neural networks with [backpropagation](/01-deep-neural-networks/01-dnn/02-dnn.ipynb), gradients start to get magnified or disappear, due to the compounding effects of multiplication by sequences of large or small 
å°½ç®¡è®­ç»ƒæ›´æ·±çš„ç¥ç»ç½‘ç»œå®ç”¨åå‘ä¼ æ’­ï¼Œè¿™äº›æ¢¯åº¦å¼€å§‹æ”¾å¤§æˆ–æ¶ˆå¤±ï¼Œè¿™æ˜¯ç”±äºæ··åˆäº†ä¹˜ä»¥ä¸€äº›åˆ—çš„å¾ˆå¤§æˆ–å¾ˆå°çš„æƒé‡çš„æ•ˆæœã€‚

weights[^12].

**This is known as the vanishing and exploding gradients problem.**
è¿™è¢«ç§°ä¸ºæ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜ã€‚

It's easy to forget how prohibitive this problem was - it completely prevented the effective training of networks beyond a few layers in depth, putting a significant constraint on the size of networks.
å¾ˆå®¹æ˜“å¿˜è®°è¿™ä¸ªé—®é¢˜æ˜¯å¦‚æ­¤çš„éš¾ä»¥å¤„ç†-è¿™ä¸ªé—®é¢˜å¯¼è‡´ç½‘ç»œæ— æ³•æœ‰æ•ˆè®­ç»ƒï¼Œè¶…è¿‡äº†ç½‘ç»œæ·±åº¦é—®é¢˜ç‚¹ç‚¹ï¼Œæ˜¯å…³äºç½‘ç»œå¤§å°çš„ä¸€ä¸ªæ˜æ˜¾çš„åˆ¶çº¦ã€‚

The introduction of [residuals](/02-optimization-and-regularization/03-residuals/02-residuals.ipynb) via the [ResNet](/02-optimization-and-regularization/03-residuals/01-residuals.pdf) architecture completely solved this problem by 
å‡ºç°äº†æ®‹å·®ç½‘ç»œï¼Œé€šè¿‡ResNetæ¶æ„ï¼Œå®Œå…¨çš„è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ
creating _residual pathways_ for gradients to flow effectively through networks of arbitrary depth.
é€šè¿‡æ„å»ºå‚ä¸çš„å°è·¯ï¼Œå¯ä»¥ä½¿å¾—æ¢¯åº¦æµé«˜æ•ˆçš„ç”¨åœ¨ä»»æ„æ·±åº¦çš„ç½‘ç»œä¸­ã€‚


This unlock removed a significant constraint on network depth, enabling much larger networks to be trained (which removed a cap on parameters that existed for a long time before this).
è¿™ä¸ªè§£é”ç§»é™¤äº†åœ¨ç½‘ç»œæ·±åº¦ä¸Šçš„çº¦æŸï¼Œä½¿å¾—æ˜æ˜¾æ›´å¤§çš„ç½‘ç»œèƒ½è¢«è®­ç»ƒï¼ˆè¿™ç§»é™¤äº†åœ¨å‚æ•°ä¸Šçš„éšœç¢ï¼Œè¿™ä¸ªé—®é¢˜å­˜åœ¨å¾ˆé•¿æ—¶é—´ï¼‰

<br />

[^12]: Understanding this section relies on a basic understanding of the fundamentals of the backpropagation algorith. [3blue1brown's neural network series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) is an 
       ç†è§£è¿™ä¸€éƒ¨åˆ†çš„å†…å®¹ï¼Œä¾èµ–äºä¸€ä¸ªå…³äºåå‘ä¼ æ’­ç®—æ³•çš„åŸºç¡€æ€§çš„åŸºæœ¬ç†è§£ã€‚3bluebrownçš„ç¥ç»ç½‘ç»œç³»åˆ—è¯¾ç¨‹æ˜¯ä¸€ä¸ªç›´è§‚å’Œæœ‰è¶£çš„ä»‹ç»ï¼Œå¦‚æœä½ æƒ³äº†è§£ç»†èŠ‚çš„è¯ã€‚
intuitive and interesting introduction for anyone who wants to learn.

### Breakthrough #2: Network of Networks
###çªç ´ #2ï¼šç½‘ç»œä¸­çš„ç½‘ç»œ

[Dropout](/02-optimization-and-regularization/04-dropout/) introduced a critical regularization strategy that has been used in most networks after it's creation, notably contributing to the success of [AlexNet](/01-deep-neural-
æå‡ºæ“ä½œå¼•å…¥äº†ä¸€ä¸ªå…³é”®çš„æ­£åˆ™åŒ–çš„ç­–ç•¥ï¼Œåœ¨å…¶è¢«å‘æ˜ä¹‹åè¢«ç”¨åˆ°äº†å¤§éƒ¨åˆ†çš„ç½‘ç»œä¸­ï¼Œä½¿å¾—AlexNnetå–å¾—äº†æˆåŠŸï¼Œ
networks/03-alex-net/02-alex-net.ipynb) which initially popularized it.
å› ä¸ºå…¶åœ¨ä¸€å¼€å§‹ç”¨åˆ°ã€‚

Conceptually, the ideal way to prevent a model from overfitting to a particular problem would be to train a variety of neural networks on the same problem and then take the average of their predictions. This process would cancel out the 
æ¦‚å¿µä¸Šï¼Œé¿å…ä¸€ä¸ªæ¨¡å‹å¯¹ä¸€ä¸ªç‰¹å®šé—®é¢˜å‡ºç°è¿‡æ‹Ÿåˆï¼Œå°±æ˜¯å»è®­ç»ƒä¸€ä¸ªå¤šä¸ªç¥ç»ç½‘ç»œåœ¨åŒä¸€ä¸ªé—®é¢˜ä¸Šï¼Œç„¶åå¹³å‡è¿™äº›ç½‘ç»œçš„é¢„æµ‹ç»“æœã€‚è¿™ä¸ªè¿‡ç¨‹å°†æ¶ˆé™¤

noise fitted by each network, leaving only the true representations.
åœ¨æ¯ä¸€ä¸ªç½‘ç»œä¸Šçš„å™ªå£°æ‹Ÿåˆï¼Œä»…ä»…å¾—åˆ°æ­£ç¡®çš„è¡¨å¾ã€‚

However, this naive approach was prohibitively expensive - training multiple large neural networks for a single problem costs more compute.
ä½†æ˜¯ï¼Œè¿™ä¸ªåŸå§‹çš„æ–¹æ³•çš„å¼€é”€éš¾ä»¥æ‰¿å—-è®­ç»ƒå¤šä¸ªå¤§å‹çš„ç¥ç»ç½‘ç»œç”¨äºä¸€ä¸ªå•ä¸€çš„é—®é¢˜ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ã€‚


Dropout enabled a computationally effective equivalent approach involving randomly blocking out the effects of a subset of neurons in each training run[^13], effectively training an exponential number of sub-networks within a neural 
å‰”é™¤æ“ä½œèƒ½åšåˆ°è®¡ç®—æ•ˆæœçš„æ–¹æ³•ç­‰ä»·ï¼Œé€šè¿‡åœ¨æ¯ä¸€æ¬¡è®­ç»ƒè¿è¡Œä¸­ï¼Œéšæœºçš„å»é™¤æ‰ä¸€ä¸ªç¥ç»å…ƒå­é›†çš„å½±å“æ–¹å¼ï¼Œæœ‰æ•ˆçš„åœ¨ç¥ç»ç½‘ç»œä¸­è®­ç»ƒä¸€ä¸ªæŒ‡æ•°æ•°é‡çš„å­ç½‘çš„æ–¹å¼ï¼Œ

network and averaging their predictions together.
å¹¶å°†è¿™äº›å­ç½‘çš„é¢„æµ‹ç»“æœå…±åŒå¹³å‡ã€‚

<br />

[^13]: This effect forces individual neurons to learn general representations useful in collaboration with a variety of other neurons, rather than co-adapting with neighboring neurons, which allows large groups of neurons to fit to 
è¿™ä¸ªæ•ˆæœæ˜¯å»å¼ºåˆ¶ç‹¬ç«‹çš„ç¥ç»å…ƒå»å­¦ä¹ é€šç”¨è¡¨å¾ï¼Œèƒ½è¢«ç”¨äºå’Œå…¶å®ƒä¸åŒçš„ç¥ç»å…ƒè¿›è¡Œåä½œï¼Œè€Œä¸æ˜¯ç”¨æ¥é€‚åº”æ€§è°ƒæ•´ç›¸é‚»çš„ä¸€äº›ç¥ç»å…ƒï¼Œåè€…å°†è®©å¾ˆå¤šåˆ†ç»„çš„ç¥ç»å…ƒå»æ‹Ÿåˆå™ªå£°ã€‚
noise.

### Breakthrough #3: Taming Activations
### çªç ´ #3ï¼šæ§åˆ¶æ¿€æ´»


Another problem when training deep networks is that later layers suffer from improving while the activations of earlier layers change, potentially rendering their early stages of training useless.
å¦ä¸€ä¸ªè®­ç»ƒç¥ç»ç½‘ç»œçš„é—®é¢˜æ˜¯ï¼Œæ¿€æ´»å‰é¢çš„ç½‘ç»œå˜åŒ–ï¼Œå½“å¯¼è‡´åé¢çš„ç½‘ç»œå±‚æ— æ³•æå‡ï¼Œæ½œåœ¨çš„è¯´æ˜åœ¨è®­ç»ƒçš„æ—©æœŸç¯èŠ‚æ¸²æŸ“æ˜¯æ²¡ä»€ä¹ˆç”¨çš„ã€‚


**This problem is known as internal covariate shift**, and also prohibitted the training of deeper networks.
è¿™ä¸ªé—®é¢˜è¢«ç§°ä¸ºå†…éƒ¨åå˜é‡åç§»ï¼Œä¹Ÿå¯¼è‡´æ— æ³•è®­ç»ƒæ›´æ·±çš„ç½‘ç»œã€‚


The introduction of [Batch Normalization](/02-optimization-and-regularization/05-batch-norm/) and [Layer Normalization](/02-optimization-and-regularization/06-layer-norm/) solved this by forcing neuron activations into predictable 
é€šè¿‡å¼•å…¥æ‰¹æ¬¡å½’ä¸€åŒ–å’Œç½‘ç»œå±‚å½’ä¸€åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶ç¥ç»å…ƒæ¿€æ´»ç”¨åœ¨å¯é¢„æµ‹çš„åˆ†å¸ƒ

distributions, preventing the covariate shift problem..
ä¸­è§£å†³äº†è¿™ä¸ªåå˜é‡åç§»çš„é—®é¢˜ã€‚


This breakthrough, combined with residuals, provided the basis for building much deeper networks. Layer Normalization in particular enabled the training of deeper reccurent models like [RNNs](/03-sequence-modeling/01-rnn/02-rnn.ipynb) 
è¿™ä¸ªçªç ´ï¼Œå¹¶ç»„åˆäº†æ®‹å·®æ–¹æ³•ï¼Œæä¾›äº†æ„å»ºæ›´æ·±ç½‘ç»œçš„åŸºç¡€ã€‚ç½‘ç»œå±‚å½’ä¸€åŒ–ç‰¹åˆ«æ˜¯èƒ½ç”¨åœ¨æ›´æ·±çš„å¾ªç¯æ¨¡å‹ï¼Œ
and [LSTMs](/03-sequence-modeling/02-lstm/02-lstm.ipynb)'s that led to the innovations eventually resulting in the [Transformer](/04-transformers/01-transformer/02-transformer.ipynb).
å¦‚RNNå’ŒLSTMï¼Œè¿™æ ·æœ€ç»ˆå‘æ˜å¾—åˆ°Transformerã€‚

<br />

### Breakthrough #4: Momentum
### çªç ´ #4ï¼šåŠ¨é‡


The initial optimization algorithm, _stochastic gradient-descent_, involves taking a pre-determined step to update the parameters at each time-step.
åˆå§‹åŒ–ä¼˜åŒ–ç®—æ³•ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ï¼Œæ¶‰åŠä½¿ç”¨ä¸€ä¸ªé¢„å…ˆç¡®å®šçš„æ­¥éª¤å»æ›´æ–°æ¯ä¸€ä¸ªæ—¶é—´æ­¥éª¤ä¸Šçš„å‚æ•°ã€‚


In practice, this can be highly inefficient and hurt convergence[^14].
å®é™…ä¸Šï¼Œè¿™æ ·åšæ˜¯éå¸¸ä½æ•ˆå¹¶ä¸åˆ©äºæ”¶æ•›ã€‚


The [Adam](/02-optimization-and-regularization/08-adam/02-adam.ipynb) optimizer introduced an efficient algorith to keep track of **adaptive moments** tracking the history of gradients throughout the optimization process. This allowed 
Adam ä¼˜åŒ–å™¨åŒ…å«äº†ä¸€ä¸ªé«˜æ•ˆçš„ç®—æ³•ï¼Œèƒ½ç¡®ä¿è¿½è¸ªâ€œè‡ªé€‚åº”åŠ¨é‡â€ï¼Œå»è¿½è¸ªæ¢¯åº¦åœ¨æ•´ä¸ªä¼˜åŒ–çš„è¿‡ç¨‹ä¸­çš„å†å²ã€‚

the optimizer to adjust step-sizes based on past information, often leading to much faster convergence.
è¿™ä½¿å¾—ä¼˜åŒ–å™¨è°ƒæ•´å•æ­¥çš„å¤§å°æ˜¯åŸºäºè¿‡å»çš„ä¿¡æ¯ï¼Œé€šå¸¸èƒ½åšåˆ°æ˜æ˜¾æ›´å¿«çš„æ”¶æ•›ã€‚


[^14]: Specifically in parameter spaces with large variance in the gradients, a certain step-size may cause over-adjustments in certain parts of the landscape, and result in painfully slow changes in other cases.
ç‰¹åˆ«æ˜¯åœ¨å‚æ•°ç©ºé—´ä¸­æœ‰å¾ˆå¤§çš„æ–¹å·®å‡ºç°åœ¨æ¢¯åº¦ä¸­ï¼Œä¸€ä¸ªç¡®å®šçš„å•æ­¥å¤§å°å¯èƒ½å¯¼è‡´è¿‡åº¦è°ƒæ•´å‡ºç°åœ¨æŸå¤±é£æ™¯çš„ç¡®å®šéƒ¨åˆ†ï¼Œå¹¶å¯¼è‡´ç—›è‹¦ç¼“æ…¢çš„å˜åŒ–å‡ºç°åœ¨å…¶å®ƒçš„æƒ…å†µã€‚


<br />

### The Forgotten Constraint
é—å¿˜çº¦æŸ

The advancements mentioned above (and related developments) are all used in most models to date. For example, the [Transformer](/04-transformers/01-transformer/02-transformer.ipynb) 
ä¹‹å‰æåˆ°çš„ä¼˜åŠ¿ï¼ˆå’Œç›¸å…³çš„è¿›å±•ï¼‰éƒ½æ˜¯è¢«ç”¨åœ¨å¤§éƒ¨åˆ†ä»Šå¤©çš„æ¨¡å‹ä¸­ã€‚

architecture uses [Dropout](/02-optimization-and-regularization/04-dropout/), [Layer Normalization](/02-optimization-and-regularization/06-layer-norm/02-layer-norm.ipynb), and[Residuals]
ä¾‹å¦‚ï¼ŒTransformeræ¶æ„ç”¨äº†Dropoutï¼ŒLayer Normalizationï¼Œå’ŒResidualsåœ¨å…¶æ¶æ„ä¸­ï¼Œ

(/02-optimization-and-regularization/03-residuals/02-residuals.ipynb) throughout it's architecture, and was trained using the [Adam](/02-optimization-and-regularization/08-adam/)
å¹¶ä¸”åœ¨è®­ç»ƒä¸­ç”¨åˆ°äº†Adam

optimizer.
ä¼˜åŒ–å™¨ã€‚



Because of how effective they've been completely removing prior problems, optimization & regularization appear to be largely solved now.
å› ä¸ºä¹‹å‰çš„é—®é¢˜å·²ç»è¢«å®Œæˆå¾ˆæœ‰æ•ˆçš„è§£å†³ï¼Œä¼˜åŒ–å’Œæ­£åˆ™åŒ–ç°åœ¨å°±è¢«å¾ˆå¤§ç¨‹åº¦ä¸Šè§£å†³äº†ã€‚

This is especially augmented by the fact that we're far from reaching the peak of the scaling laws on current internet-scale datasets, so overfitting is not a concern.
è¿™ç§æƒ…å†µç‰¹åˆ«æ˜¯è¢«ç°å®æ‰€å¼ºåŒ–ï¼Œå› ä¸ºå½“å‰ç”¨åˆ°çš„ç¼©æ”¾æ³•åˆ™è¿˜è¿œè¿œæ²¡åˆ°æŠµè¾¾äº’è”ç½‘è§„æ¨¡çš„æ•°æ®é›†çš„å³°å€¼ï¼Œäºæ˜¯è¿‡æ‹Ÿåˆçš„é—®é¢˜ä¸è¢«å…³æ³¨ã€‚

**Despite this, it's important to remember that optimization & regularization are still real constraints on the size of neural networks**, although they no longer effect models in their 
å°½ç®¡å¦‚æ­¤ï¼Œå¾ˆé‡è¦æ˜¯è¦è®°ä½ï¼Œä¼˜åŒ–å’Œæ­£åˆ™åŒ–ä»ç„¶æ˜¯ç¥ç»ç½‘ç»œä¸Šå®å®åœ¨åœ¨çš„çº¦æŸï¼Œå°½ç®¡è¿™äº›åœ¨å½“å‰çŠ¶æ€å·²ç»ä¸å†å½±å“æ¨¡å‹ã€‚
current state.

<br />

## 1.4. Architecture
## 1.4. æ¶æ„

![constraint-4-architecture](./images/readme/constraint-4-architecture.png)
çº¦æŸ-4-æ¶æ„

We covered how increasing the number of parameters in a neural network increases its _representational capacity_. This can be understood as the networks ability to store _useful 
æˆ‘ä»¬è®ºè¿°äº†å¦‚ä½•å¢åŠ ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°é‡æ¥å¢åŠ å…¶è¡¨å¾å®¹é‡ã€‚è¿™ä¸ªæ–¹æ³•èƒ½è¢«ç†è§£ä¸ºå¢å¼ºç½‘ç»œä¿å­˜æœ‰ç”¨è¡¨å¾çš„èƒ½åŠ›ï¼Œ

representations_ that effectively model the empirical distribution.
äºæ˜¯å¯ä»¥å»ºæ¨¡ç»éªŒåˆ†å¸ƒã€‚

By default, deep neural networks are forced to learn the most optimal ways to store representations for different problems.
é»˜è®¤æƒ…å†µï¼Œæ·±åº¦ç¥ç»ç½‘ç»œæ˜¯è¢«å¼ºåˆ¶å»å­¦ä¹ æœ€ä¼˜è·¯å¾„å»ä¿å­˜è¡¨å¾ï¼Œç”¨äºä¸åŒçš„é—®é¢˜ã€‚

However, when we already know an effective method for the model to store useful representations relevant to a particular problem, it can be helpful to build the ability to store 
ä½†æ˜¯ï¼Œå½“æˆ‘ä»¬å·²ç»çŸ¥é“ä¸€ä¸ªæœ‰æ•ˆçš„æ–¹æ³•ç”¨äºæ¨¡å‹å»ä¿å­˜æœ‰ç”¨çš„è¡¨å¾å»å…³è”åˆ°ä¸€ä¸ªç‰¹å®šçš„é—®é¢˜ï¼Œè¿™æ ·æœ‰åŠ©äºå»æ„å»ºä¿å­˜è¡¨å¾åˆ°è¿™ä¸ªæœ‰ç”¨å½¢å¼çš„èƒ½åŠ›ï¼Œç›´æ¥ç»™æ¨¡å‹ã€‚
representations in this useful form directly into the model.

**Building specific structures into the neural network design to make it easier for the model to store useful representations is known as adding inductive bias.**
æ„å»ºç‰¹å®šç»“æ„åˆ°ç¥ç»ç½‘ç»œçš„è®¾è®¡ä¸­ï¼Œä½¿å¾—èƒ½æ›´å®¹æ˜“çš„è®©æ¨¡å‹å»ä¿å­˜æœ‰ç”¨çš„è¡¨å¾ï¼Œè¿™è¢«ç§°ä¸ºåŠ å…¥å½’çº³åç½®ã€‚

Desiging good neural network architectures into our models is about increasing the density of _useful representations_ in the model, meaning more efficient usage of parameters.
è®¾è®¡å¥½çš„ç¥ç»ç½‘ç»œæ¶æ„ç»™æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå°±æ˜¯å…³äºå¢åŠ æ¨¡å‹ä¸­æœ‰ç”¨è¡¨å¾çš„å¯†åº¦ï¼Œæ„å‘³ç€æ›´é«˜æ•ˆçš„ä½¿ç”¨å‚æ•°ã€‚

In this way, improved architectures can achieve similar effects to scaling up parameters.
äºæ˜¯ï¼Œæå‡åçš„æ¶æ„èƒ½å®ç°ä¸å¢åŠ å‚æ•°æ–¹æ³•ç±»ä¼¼çš„æ•ˆæœã€‚


In practice, architectural advancements have made previously intractable problems (like image synthesis) possible for neural networks.
å®é™…ä¸Šï¼Œæ¶æ„æå‡å·²ç»ä½¿å¾—å‰ä¸€ä¸ªéš¾ä»¥å¤„ç†çš„é—®é¢˜ï¼ˆç±»ä¼¼å›¾ç‰‡åˆæˆï¼‰åœ¨ç¥ç»ç½‘ç»œä¸­å¯è¡Œã€‚

> [!NOTE]
>
> **Constraint #4: The quality of the network architecture constrains the representational capacity of a model.**
>çº¦æŸ #4ï¼šç½‘ç»œæ¶æ„çš„è´¨é‡çº¦æŸäº†æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚

Technically, a deep neural network with non-linearities is capable of modeling any distribution, given a sufficient number of parameters[^15].
æŠ€æœ¯ä¸Šï¼Œä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œå…¶éçº¿æ€§çš„ç‰¹æ€§æ˜¯èƒ½å»å»ºæ¨¡ä»»æ„çš„åˆ†å¸ƒï¼Œå½“å‰è¦åŸºäºç»™å®šçš„è¶³å¤Ÿå‚æ•°æ•°é‡ã€‚


But in practicality, there are distributions with so much complexity that simple deep neural networks can't effectively model them[^16]. For these distributions, we turn to architectural 
ä½†æ˜¯åœ¨æŸäº›ç‰¹æ®Šæƒ…å†µï¼Œæœ‰ä¸€äº›åˆ†å¸ƒåŒ…å«äº†éå¸¸å¤§çš„å¤æ‚æ€§ï¼Œå¯¼è‡´ç®€å•çš„æ·±åº¦ç¥ç»ç½‘ç»œæ— æ³•æœ‰æ•ˆçš„å»ºæ¨¡ã€‚å¯¹äºè¿™äº›åˆ†å¸ƒï¼Œæˆ‘ä»¬å°±å»åšæ¶æ„æå‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
advancements to make progress.

<br />

[^15]: This idea is explored in the original [backpropagation paper](/01-deep-neural-networks/01-dnn/01-dnn.pdf).
è¿™ä¸ªæƒ³æ³•æœ€å¼€å§‹åœ¨åå‘ä¼ æ’­çš„è®ºæ–‡ä¸­æåˆ°ã€‚

[^16]: For example, image classification, where individual pixel values are noisy and subject to a variety of transformations.
ä¾‹å¦‚ï¼Œå›¾ç‰‡åˆ†ç±»ï¼Œå…¶ä¸­çš„ç‹¬ç«‹åƒç´ å€¼æ˜¯æœ‰å™ªå£°çš„ï¼Œå¹¶ä¸”æœä»ä¸€ä¸ªä¸åŒçš„å˜æ¢å½¢å¼ã€‚

### Breakthrough #1: Learning Features
### çªç ´ #1ï¼šå­¦ä¹ ç‰¹å¾

The [Convolutional Neural Network](/01-deep-neural-networks/02-cnn/03-cnn.ipynb) was the first effective architecture that introduced a significant inductive bias into neural networks. 
è¿™ä¸ªå·ç§¯ç¥ç»ç½‘ç»œæ˜¯ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„æ¶æ„ï¼Œå…¶å¼•å…¥äº†ä¸€ä¸ªæ˜¾è‘—çš„å½’çº³åç½®åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚

The idea behind the CNN is directly inspired by the hierarchical processing of inputs from the brain's vision system.
CNNèƒŒåçš„æ€æƒ³æ˜¯ç›´æ¥å—åˆ°å±‚æ¬¡ç»“æ„è¾“å…¥è¿‡ç¨‹çš„å¯å‘ï¼Œè¿™æ¥è‡ªå¤§è„‘è§†è§‰ç³»ç»Ÿã€‚

CNNs use _feature maps_ that detect high-level features across images to implement the translational invariance that's critical to image recognition tasks.
CNNç”¨äº†ç‰¹å¾å›¾ï¼Œå…¶æ¢æµ‹å›¾ç‰‡ä¸­çš„é«˜å±‚çº§çš„ç‰¹å¾ï¼Œå»å®ç°å¹³ç§»ä¸å˜æ€§ï¼Œè¿™æ˜¯å›¾ç‰‡è¯†åˆ«ä»»åŠ¡çš„å…³é”®ã€‚

This provided a deep learning analogue to the manual feature engineering efforts often used before deep learning was proven.
è¿™æ ·å¾—åˆ°çš„ç¥ç»ç½‘ç»œç±»ä¼¼äºæ‰‹å·¥ç‰¹å¾å·¥ç¨‹ï¼Œè¿™é€šå¸¸è¢«ç”¨åœ¨æ·±åº¦å­¦ä¹ æµè¡Œä¹‹å‰çš„å·¥ä½œä¸­ã€‚

CNNs were critical for the initial adoption of deep learning - neural networks like [LeNet](/01-deep-neural-networks/02-cnn/02-le-net.pdf) and [AlexNet](/01-deep-neural-networks/03-alex-
CNNæ˜¯æœ€å¼€å§‹åº”ç”¨æ·±åº¦å­¦ä¹ çš„å…³é”®å·¥ä½œï¼Œç¥ç»ç½‘ç»œç±»ä¼¼LeNetå’ŒAlexNet

net/01-alex-net.pdf) used the architecture to beat the state-of-the-art in image classification competitions. Additionally CNNs are still relevant in modern models with the [U-Net](/01-
ç”¨åˆ°äº†è¿™ä¸ªæ¶æ„ï¼Œæ‰“è´¥äº†å›¾ç‰‡åˆ†ç±»ç«èµ›ä¸­çš„æœ€å¥½çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒCNNsä»ç„¶æ˜¯ä¸ç°ä»£æ¨¡å‹ç›¸å…³çš„ï¼ŒåŒ…æ‹¬U-Netæ¶æ„è¢«ç”¨åœ¨ç°ä»£æ¨¡å‹ä¸­ç”¨äºå›¾ç‰‡ç”Ÿæˆã€‚

deep-neural-networks/04-u-net/02-u-net.ipynb) architecture being used in modern [Diffusion](/05-image-generation/03-diffusion/05-diffusion.ipynb) models for image generation.

<br />

### Breakthrough #2: Memory
###çªç ´ # 2 ï¼šå†…å­˜


The [Recurrent Neural Network](/03-sequence-modeling/01-rnn/02-rnn.ipynb) introduced the ability to store memories about the past to inform future decisions.
å¾ªç¯ç¥ç»ç½‘ç»œå¼•å…¥äº†ä¸€äº›èƒ½åŠ›ï¼Œå¯ä»¥ä¿å­˜è®°å¿†æ˜¯å…³äºè¿‡å»ï¼Œå¯ä»¥å¼ºåŒ–æœªæ¥çš„å†³ç­–ã€‚

While theoretically interesting, it remained largely ineffective for sequence-modeling tasks until the introduction of the [Long Short-Term Memory](/03-sequence-modeling/02-lstm/02-
å°½ç®¡ç†è®ºä¸Šå¾ˆæœ‰æ„æ€ï¼Œå…¶æœ‰å¾ˆå¤§çš„æ— æ•ˆæ€§åœ¨å­¦åˆ—å»ºæ¨¡çš„ä»»åŠ¡ä¸Šå‘ç”Ÿï¼Œç›´åˆ°å‡ºç°äº†é•¿çŸ­æ—¶è®°å¿†æ¶æ„ï¼Œ

lstm.ipynb) architecture which enabled neural networks to learn complex relationships across time and space by learning to store, retrieve, and [forget](/03-sequence-modeling/03-learning-
å…¶ä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ å¤æ‚çš„å…³ç³»è·¨è¶Šæ—¶é—´å’Œç©ºé—´ï¼Œé€šè¿‡å­¦ä¼šä¿å­˜ï¼Œ
to-forget/02-learning-to-forget.ipynb) memories over long time horizons.
æ£€ç´¢å’Œé—å¿˜è®°å¿†çš„æ–¹æ³•åœ¨ä¸€ä¸ªé•¿æ—¶é—´çš„èŒƒå›´ã€‚

**The LSTM inductive bias made them effective at sequence-modeling tasks, kicking off the arc of progress that eventually led to the creation of the Transformer.**
è¿™ä¸ªLSTMå½’çº³åç½®ä½¿å¾—å…¶èƒ½æœ‰æ•ˆçš„ç”¨åœ¨åºåˆ—å»ºæ¨¡çš„ä»»åŠ¡ä¸Šï¼Œæ‹‰å¼€äº†è¿›æ­¥çš„å¼§çº¿ï¼Œæœ€ç»ˆä¸ºTransformerå‡ºç°å¥ å®šäº†åŸºç¡€ã€‚

Despite their efficacy, the LSTM was constrained by the fact that it processed input sequences sequentially, making it slow to train.
å°½ç®¡è¿™ä¸ªæ–¹æ³•æœ‰æ•ˆï¼Œä½†æ˜¯LSTMäº‹å®ä¸Šå—é™äºå…¶å¤„ç†è¾“å…¥åºåˆ—çš„é¡ºåºæ€§è¦æ±‚ï¼Œä½¿å¾—å…¶è®­ç»ƒå¾ˆæ…¢ã€‚

<br />

### Breakthrough #3: Attention
### çªç ´ #3ï¼šæ³¨æ„åŠ›

The [Attention](/03-sequence-modeling/06-attention/02-attention.ipynb) mechanism was initially introduced as an addition to LSTMs to enhance their ability to understand the relationship 
æ³¨æ„åŠ›æœºåˆ¶æ˜¯æœ€åˆä½œä¸ºLSTMçš„ä¿®è¡¥å‡ºç°ï¼Œä¸ºäº†å¢å¼ºå…¶èƒ½åŠ›å»ç†è§£æ¦‚å¿µä¹‹é—´çš„å…³ç³»ã€‚
between concepts.

The now famous [_Attention Is All You Need_](/04-transformers/01-transformer/01-transformer.pdf) paper removed all the LSTM components and demonstrated that the inductive bias of 
ç°åœ¨æœ‰åçš„æ³¨æ„åŠ›æ˜¯æ‰€æœ‰æ‰€éœ€è¦çš„è¿™ç¯‡è®ºæ–‡ç§»é™¤äº†å…¨éƒ¨çš„LSTMéƒ¨åˆ†ï¼Œ
attention alone is effective for sequence-modeling tasks, introducing the [Transformer](/04-transformers/01-transformer/02-transformer.ipynb) architecture which has permanently changed 
å¹¶éªŒè¯äº†å½’çº³åç½®çš„æ³¨æ„åŠ›å•ç‹¬æ˜¯å¯ä»¥æœ‰æ•ˆçš„ç”¨äºåºåˆ—å»ºæ¨¡ä»»åŠ¡ï¼Œå¼•å…¥äº†Transformeræ¶æ„æ·±è¿œçš„æ”¹å˜äº†æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚
deep learning.


**The transformer is particularly effective not just because of the power of the attention mechanism, but because of the high parallelization it achieved by removing recurrence.**
Transformeræ˜¯ç‰¹åˆ«æœ‰æ•ˆï¼Œä¸ä»…ä»…å› ä¸ºå…¶æ³¨æ„åŠ›æœºåˆ¶çš„èƒ½åŠ›ï¼Œä¹Ÿå› ä¸ºå…¶é«˜åº¦çš„å¹¶å‘æ€§ï¼Œè¿™æ˜¯é€šè¿‡å‰”é™¤äº†å¾ªç¯å®ç°çš„ã€‚


<br />

### Breakthrough #4: Harnessing Randomness
### çªç ´ #4 åˆ©ç”¨éšæœºæ€§
The CNN introduced the ability to understand samples from the complex distribution of images.
CNNå¼•å…¥äº†ä¸€ç§èƒ½åŠ›ï¼Œå»ç†è§£å›¾ç‰‡ä¸­å¤æ‚åˆ†å¸ƒçš„æ ·æœ¬ã€‚

However, the problem of synthesizing images appeared to be much harder - CNNs could learn to filter out the details in images and focus on high-level features, whereas image geneartion models would need to learn to create both high-level features and complex details.
ä½†æ˜¯ï¼Œåˆæˆå›¾ç‰‡ä¸­çš„é—®é¢˜å°†å˜å¾—æ›´éš¾-CNNæ˜¯èƒ½å­¦ä¹ å»è¿‡æ»¤å‡ºå›¾ç‰‡ä¸­çš„ç»†èŠ‚ï¼Œå¹¶èšç„¦åœ¨é«˜å±‚çš„ç‰¹å¾ä¸Šï¼Œç”±äºå›¾ç‰‡ç”Ÿæˆæ¨¡å‹å°†éœ€è¦å»å­¦ä¹ åŒæ—¶åˆ›å»ºé«˜å±‚çš„ç‰¹å¾å’Œå¤æ‚çš„ç»†èŠ‚ã€‚

Image generation models like [Variational Auto-Encoders](/05-image-generation/02-vae/04-vae.ipynb) and [Diffusion](/05-image-generation/03-diffusion/05-diffusion.ipynb) models learn to generate both high-level features and complex details by introducing random sampling and noise directly into their architectures.
å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ï¼Œç±»ä¼¼å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ å»åŒæ—¶ç”Ÿæˆé«˜å±‚çš„ç‰¹å¾å’Œå¤æ‚çš„ç»†èŠ‚ï¼Œé€šè¿‡å¼•å…¥éšæœºé‡‡æ ·å’Œå™ªå£°ï¼Œç›´æ¥ç”¨åˆ°è¿™ä¸ªæ¶æ„ä¸­ã€‚

VAEs create a bottleneck that forces the models to learn useful representations in a low dimensional space. Then, they add back noise on top of these representations through random sampling. **So VAEs start by learning representations, and then add noise.**
VAEåˆ›å»ºäº†ä¸€ä¸ªç“¶é¢ˆï¼Œå¼ºåˆ¶æ¨¡å‹å»å­¦ä¹ æœ‰ç”¨çš„è¡¨å¾ä½çº¬åº¦ç©ºé—´ã€‚ç„¶åï¼Œå…¶åŠ å›å™ªå£°åˆ°è¿™äº›è¡¨å¾çš„é¡¶éƒ¨ï¼Œé€šè¿‡éšæœºé‡‡æ ·çš„æ–¹å¼ã€‚â€œè¿™æ ·VAEå¼€å§‹æ˜¯é€šè¿‡å­¦ä¹ è¡¨å¾ï¼Œå¹¶éšå³åŠ ä¸Šå™ªå£°â€

**Diffusion models, instead, starts with noise, and learn to add information into to the noise slowly.**
æ‰©æ•£æ¨¡å‹ï¼Œè€Œæ˜¯ï¼Œå¼€å§‹ç”¨åˆ°å™ªå£°ï¼Œå¹¶å­¦ä¹ å»ç¼“æ…¢åŠ å…¥ä¿¡æ¯åˆ°å™ªå£°ã€‚

Without these designs, modern image generation models like [Stable Diffusion](https://arxiv.org/abs/2112.10752) and [DALL E](/05-image-generation/05-dall-e/) wouldn't exist.
æ²¡æœ‰è¿™äº›ä¸–ç•Œï¼Œç°ä»£çš„å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ï¼Œç±»ä¼¼ç¨³å®šæ‰©æ•£å’ŒDALLEå°†ä¸ä¼šæ‘å­ã€‚

<br />

### Breakthrough #5: Embeddings
### çªç ´ #5ï¼šåµŒå…¥

The [Word2Vec](/03-sequence-modeling/04-word2vec/03-word2vec.ipynb) model popularized the concept of text embeddings that preserve semantic and syntactic meaning by forcing models to create vector representations for concepts with interesting properties.
è¿™äº›Word2Vecæ¨¡å‹æ¨åŠ¨äº†æ–‡æœ¬åµŒå…¥çš„æ¦‚å¿µï¼Œè¿™æ ·ä¿ç•™äºˆä»¥å’Œå¥æ³•çš„å«ä¹‰ï¼Œé€šè¿‡å¼ºåˆ¶æ¨¡å‹å»åˆ›å»ºå‘é‡è¡¨å¾ï¼Œç”¨åœ¨ä¸€äº›æœ‰è¶£ç‰¹å¾çš„æ¦‚å¿µè¡¨å¾ã€‚

A commonly used example of the power of such embeddings is that the following equation holds true in the embedding space: Emedding("King") - Embedding("Man") + Embedding("Woman") = Embedding("Queen").
ä¸€ä¸ªé€šå¸¸è¢«ç”¨åˆ°çš„ä¾‹å­å…³äºè¿™äº›åµŒå…¥çš„èƒ½åŠ›ï¼Œæ˜¯æ²¿ç”¨äº†åŒç­‰ä¿æŒæ­£ç¡®çš„åœ¨åµŒå…¥ç©ºé—´ï¼šåµŒå…¥ï¼ˆâ€œå›½ç‹â€ï¼‰-åµŒå…¥ï¼ˆâ€œç”·äººâ€ï¼‰+åµŒå…¥ï¼ˆâ€œå¥³äººâ€ï¼‰=åµŒå…¥ï¼ˆâ€œç‹åâ€ï¼‰

Embeddings show us how the relationships between concepts can be represented in a highly condensed format.
è¿™äº›åµŒå…¥æ˜¾ç¤ºäº†åœ¨è¿™äº›æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œèƒ½è¢«è¡¨å¾ä¸ºé«˜åº¦æµ“ç¼©çš„å…³ç³»ã€‚

Later models like [CLIP](/05-image-generation/04-clip/02-clip.ipynb) based on the [Transformer](/04-transformers/07-vision-transformer/02-vision-transformer.ipynb) architecture have led to complex embedding spaces mapping understandings of concepts across modalities to a single representation space, enabling multi-modal models like [DALL E 2](/05-image-generation/05-dall-e/02-dall-e-2.pdf).
éšåï¼Œç±»ä¼¼CLIPçš„æ¨¡å‹åŸºäºTransformeræ¶æ„ï¼Œå·²ç»å¾—åˆ°äº†å¤æ‚çš„åµŒå…¥ç©ºé—´æ˜ å°„ï¼Œä»åŸºäºå¤šä¸ªæ¨¡æ€ç†è§£çš„æ¦‚å¿µæ˜ å°„åˆ°å•ä¸€è¡¨å¾çš„ç©ºé—´ï¼Œèƒ½ä½¿å¾—å¤šæ¨¡æ€çš„æ¨¡å‹ï¼Œç±»ä¼¼DALL E 2ã€‚

<br />

### "Don't Touch the Architecture"
### â€œä¸è¦è§¦ç¢°è¿™äº›æ¶æ„â€

For the past several years after the introduction of the [Transformer](/04-transformers/01-transformer/02-transformer.ipynb), efforts have mainly been focused around scaling up the parameters and data fed into transformers without heavily adjusting the inductive biases.
å¯¹äºåœ¨å¼•å…¥äº†Transformerä¹‹åè¿‡å»çš„ä¸€äº›å¹´ï¼Œä¸€äº›åŠªåŠ›ä¸»è¦èšç„¦åœ¨æ”¾å¤§å‚æ•°å’Œæ•°æ®å»ä½¿ç”¨åˆ°Transformerä¸­ï¼Œæ²¡æœ‰é‡å¤§çš„å»è°ƒæ•´å½’çº³åç½®ã€‚

This suggests a stagnation in architectural improvement motivated by the efficacy of the Transformer, which may suggest something about the inherent efficacy of the inductive bias of [Attention](/03-sequence-modeling/06-attention/) in intelligence.
è¿™æ ·ä½¿å¾—åœ¨æå‡Transformeræ•ˆç‡æ¥é©±åŠ¨çš„æ¶æ„æå‡åœæ»ä¸å‰ï¼Œè¿™æ ·å¯èƒ½è¯´æ˜äº†æ™ºèƒ½ä¸­æ³¨æ„åŠ›çš„å½’çº³åç½®çš„æŸäº›å›ºæœ‰çš„æ•ˆç‡ã€‚

**This explicit desire not to change architectures anymore is [discussed by Andrej Karpathy in this clip](https://www.youtube.com/watch?v=9uw3F6rndnA).**
è¿™ä¸ªæ˜¾ç¤ºçš„æ„¿æœ›ä¸ä¼šæ”¹å˜æ›´å¤šçš„æ¶æ„ï¼Œè¿™åœ¨Andrej Karparttyçš„è§†é¢‘ç‰‡æ®µä¸­è®¨è®ºäº†ã€‚

Instead of changing base architectures, many state-of-the-art models have been combining different existing architectures together - for example, the [Diffusion](/05-image-generation/03-diffusion/05-diffusion.ipynb) model design uses the [U-Net](/01-deep-neural-networks/04-u-net/02-u-net.ipynb) underneath, and [DALL-E-2](/05-image-generation/05-dall-e/02-dall-e-2.pdf) uses both [CLIP](/05-image-generation/04-clip/02-clip.ipynb) (which is built with the [Vision Transformer](/04-transformers/07-vision-transformer/02-vision-transformer.ipynb)) and a [Diffusion](/05-image-generation/03-diffusion/05-diffusion.ipynb) model.
ä¸åŒäºæ”¹å˜åŸºç¡€æ¶æ„çš„æ–¹æ³•ï¼Œå¤§éƒ¨åˆ†æœ€å¥½çš„æ¨¡å‹å·²ç»è¢«ç»„åˆåˆ°ä¸åŒç°æœ‰çš„æ¶æ„ä¸€å—ã€‚ä¾‹å¦‚ï¼Œæ‰©æ•£æ¨¡å‹è®¾è®¡ç”¨äº†U-Netä½œä¸ºåº•å±‚ï¼Œå¹¶ä¸”DALL-E-2åŒæ—¶ç”¨äº†CLIPï¼ˆå…¶æ„å»ºç”¨äº†Vision Transformerï¼‰ï¼Œå’Œä¸€ä¸ªæ‰©æ•£æ¨¡å‹ã€‚

The combination of different working architectures has also resulted in the increasing multi-modality of models, indicative in the recent [announcement of GPT-4o](https://openai.com/index/hello-gpt-4o/) which trains a single base model on a variety of modalities (likely combining a variety of architectures underneath, although the implementation details are unreleased.).
è¿™ä¸ªç»„åˆäº†ä¸åŒå·¥ä½œçš„æ¶æ„å·²ç»åœ¨ä¸æ–­å¢åŠ çš„å¤šæ¨¡æ€æ¨¡å‹å‡ºç°ï¼Œè¯´æ˜è¿‘æœŸå‘å¸ƒçš„GPT-4oè®­ç»ƒäº†å•ç‹¬ä¸€ä¸ªåŸºç¡€æ¨¡å‹åœ¨ä¸€ä¸ªå¤šæ¨¡æ€ä¸Šï¼ˆç±»ä¼¼ç»„åˆäº†ä¸€ä¸ªå¤šä¸ªåº•å±‚çš„æ¶æ„ï¼Œå°½ç®¡å®ç°çš„ç»†èŠ‚æ²¡æœ‰å…¬å¸ƒï¼‰

<br />

## 1.5. Compute

![constraint-5-compute](./images/readme/constraint-5-compute.png)

Assuming an efficient architecture and effective optimization & regularization, the last constraint on the total number of parameters and representational capacity in a model is **compute**.

During training, the gradient for each parameter needs to be computed and updated at each time-step, which costs computational resources. **So, with more parameters, there are far more computations during back-propagation which becomes the limiting step.**

Because of this, a single device can train a finite number of parameters at once, and beyond this, training has to expand to multiple devices at once to parallelize.

**And if there's a limit on the number of devices we can use for training, we hit a constraint on compute.**

So we can train a certain number of parameters per device. And then we need to get more devices. And if there's a limit on how many devices we can use together, we've hit a constraint on compute.

> [!NOTE]
>
> **Constraint #5: The total available compute constraints the maximum number of trainable parameters a model can have.**

In practice, the constraint may be caused by a lack of resources (to buy devices), supply (due to constrained supply chains), or energy (discussed later)[^17].

<br />

[^17]: There are also many engineering challenges with training on increasingly large clusters of devices like GPUs that need to be able to communicate with each other.

### Breakthrough #1: Communicating Compute

[AlexNet](/01-deep-neural-networks/03-alex-net/02-alex-net.ipynb) was one of the first major deep learning applications that took advantage of the parallelization capacity of GPUs to train neural networks.

They were also the first people to train a deep learning model across multiple GPUs at once to speed up training.

**They were able to accomplish this because of the recent addition of the ability for NVIDIA GPUs to write to each others memory**, which enabled much faster direct communication between GPUs rather than communicating through the host machine.

This innovation (introduced due to gaming, not deep learning), has become critical in training large models, where communication between large clusters of GPUs has become essential.

This paper pushed the compute constraint in several ways - first, just by using GPUs for training the first place, and additionally by using multiple GPUs to shard training, and using inter-GPU communication.

<br />

### Breakthrough #2: Riding Tailwinds

Until the past decade, the GPUs that have enabled deep learning to progress so far were driven forward not by the incentives of deep learning (which offered scarce revenue opportunity early-on for large companies like NVIDIA), but by the tailwinds of the gaming market.

In this way, deep learning benefited from a bit of luck - the compute tailwinds created by the gaming industry enabled deep learning to take off in a way that likely would not have happened in the absence of gaming.

**The gaming industry raised the constraint on compute for deep learning models by creating a sufficient financial incentive to produce GPUs of increasing quality.**

Through the trail of papers, you can see the quality of compute slowly get better over time, even before dedicated AI compute was created.

<br />

### Breakthrough #3: AI Gets Prioritized

Finally, in 2020, NVIDIA released their A100 model built specifically for AI applications, as they determined that AI was a strategic bet worth taking. This decision has now yielding the H100, and soon B100 GPUs that will power much of AI training.

<p align="center">
  <img src="/images/readme/nvidia-openai.jpeg" alt="NVIDIA x OpenAI" width="50%" />
</p>
<p align="center">
  <i>Jensen Huang delivering an H200 to early OpenAI</i>
</p>

<br />

### Breakthrough #4: The Compute Arms Race

It wasn't initially obvious that acquiring compute would become a huge constraint.

The power laws trend that first became visible with [BERT](/04-transformers/02-bert/01-bert.pdf), [RoBERTa](/04-transformers/02-bert/02-roberta.pdf), [GPT-2](/04-transformers/04-gpt/01-gpt-2.pdf), and [GPT-3](/04-transformers/04-gpt/02-gpt-3.pdf) made it clear that scaling up parameters, and thus compute, was a necessary factor of increasing model intelligence.

As this trend became more clear and the AI narrative became more powerful, everyone began to acquire the necessary compute, leading to a demand volume that wasn't previously predicted by the supply-chain. This has caused a constraint in acquiring compute.

**In addition, the raw cost of acquiring a large amount of compute has become prohibitively expensive for most players.**

These constraints on compute led [Sam Altman to say that "compute is going to be the currency of the future."](https://www.youtube.com/watch?v=r2UmOBrrRK8)

[Zuck spent several billion to buy 350,000 NVIDIA GPUs](https://www.pcmag.com/news/zuckerbergs-meta-is-spending-billions-to-buy-350000-nvidia-h100-gpus), which now appears to be an act of incredible foresight considering the current struggle to get compute.

This increased demand for compute has also been reflected in the surging market caps of all the essential companies in NVIDIA's compute supply chain including TSMC & ASML.

<br />

### Adjusting Supply Chains

The current constraint on compute is partially a result of compute supply chains not having predicted the unexpected jump in demand caused by the AI boom.

As supply chains inevitably adjust to meet these demands, the constraint will likely shift from who has already obtained the most compute to who has the resources to purchase the most compute, which also positions OpenAI well considering their partnership with the well-resourced Microsoft.

<br />

### AI ASICs

In recent fundraising cycles, many startups have raised money to build dedic-ated AI chips for inference and training, promising to further speed up the efficiency of training large models.

These specialized chips, broadly known as **Application Specific Integrated Circuits**, build assumption about how deep learning models work directly into hardware, offering the ability to drastically accelerate training.

The question is, will other companies be able to compete in this space, or will NVIDIA maintain it's domination of the AI training market (most likely).

<br />

## 1.6. Compute Efficiency

![constraint-6-compute-efficiency](./images/readme/constraint-6-compute-efficiency.png)

While the power of compute increases, making effective use of this compute is not a guarantee. Using compute efficiently is a software problem that takes active effort and optimization.

Innovations like [FlashAttention](https://arxiv.org/abs/2205.14135), which drastically accelerated the speed of Transformers through an optimization in how attention access memory, are a reminder that compute optimizations are another lever to increase the efficiency of training and scale up models.

> [!NOTE]
>
> **Constraint #6: The software implementations for training constrain the efficiency of compute utilization.**

<br />

### Breakthrough #1: CUDA

Initially, GPUs were challenging to work with as they depended on a completely new programming paradigm.

The introduction of [CUDA](https://en.wikipedia.org/wiki/CUDA) as a GPU programming paradigm familiar to C programmers made writing GPU code far more approachable.

This language enabled [AlexNet](/01-deep-neural-networks/03-alex-net/01-alex-net.pdf) to manually implement their own kernels to speed up the convolution operation on GPUs, unlocking a new level of parallelization for training CNNs.

<br />

### Breakthrough #2: Kernel Libraries

People rarely have to write low-level kernels anymore since popular libraries like [PyTorch](https://pytorch.org/) and [JAX](https://github.com/google/jax) have already written the kernel code for the most popular kernels, making it easy for modern deep learning engineers to use GPUs without needing to dip into low-level code.

<br />

### Continuous Improvement

Despite the fact that GPU kernels are now largely written, there are likely still plenty of opportunities for improving the compute efficiency of model implementations - notably, the introduction of [FlashAttenion](https://arxiv.org/abs/2205.14135) demonstrated how big of a difference these changes could make in terms of training efficiency.

<br />

## 1.7. Energy

![constraint-7-energy](./images/readme/constraint-7-energy.png)

**Finally, even if the compute supply chains are capable of supporting all demand, and we have infinite resources to purchase compute, there is still a constraint on compute: energy**

In practice, large training runs need to be run on physically clustered compute in large data centers since the devices need to communicate with each other.

As the amount of devices in large training runs grows, datacenters will need to be able to support the energy needs of these devices.

This may actually become a meaningful constraint, as [Zuck discussed in this clip on the Dwarkesh podcast](https://www.youtube.com/watch?v=i-o5YbNfmh0).

Specifically, energy grids are limited to allowing a certain amount of energy being drawn from them in a location, meaning there's a cap to how large data-centers can become before they run into problems that require energy permitting and dipping into much slower government regulated processes.

> [!NOTE]
>
> **Constraint #7: The energy available to draw from the grid in a single location constrains the amount of compute that can be used for a training run.**

As many companies plan to build large data-centers for AI training, we'll see how the energy constraint plays out - notably, [Microsfot and OpenAI are rumored to be launching a $100B data-center project](https://www.reuters.com/technology/microsoft-openai-planning-100-billion-data-center-project-information-reports-2024-03-29/).

<br />

## 1.8. Constraints & Leverage

Having covered each constraint individually, we can now put them all into perspective in relation to the broader arrow of progress in deep learning.

**A helpful way to think about the 7 constraints is in terms of _hard constraints_ and _leverage_.**

The hard constraints are **data**, **compute**, and **energy** - these are rate-limited by slow processes - data currently being limited by the scaling growth of the internet and other data collection methods, compute being limited by individual company resources and supply chains, and energy constraints eventually being rate-limited by regulation.

Meanwhile, **parameters**, **optimization & regularization**, **architecture**, and **compute efficiency** can be thought of as forms of **leverage** on the hard constraints - they are all easy to vary and can be optimized to maximize a models intelligence given a fixed set of data, compute, and energy.

**Maximizing leverage constraints are important for individual training runs, but improving the hard constraints is what really pushed forward the increasing base intelligence of models now.**

This is again indicative of the scaling laws - our models have not shown signs of coming close to fully modeling the information in current internet-scale datasets, so we continue to scale up models by increasing _compute_ and _parameters_

<br />

# 2. Narratives

We can look back at this history of progress in deep learning through the lens of constraints, and see a few key milestones that stand out above the rest which have completely shifted narratives around deep learning.

Since narratives are a powerful tool for allocating capital and talent toward problems[^18], these narrative shifts alone have had a significant impact on deep learning progress.

<br />

[^18]: For those curious, [Kevin Kwok's essay on Narrative Distillation](https://kwokchain.com/2021/09/29/narrative-distillation-1/) an excellent exploration of the power of narratives in capital and resource allocation.

### Narrative #1: Deep Learning Works

The first major narrative shift in deep learning occured after the release of [AlexNet](/01-deep-neural-networks/03-alex-net/01-alex-net.pdf) in 2012.

Prior to this paper, deep learning was considered inferior to traditional ML, as it consistently lost to manual feature engineering approaches in image classification and other challenges.

The success of AlexNet brought down the top-5 error rate on the ImageNet challenge from 25.8% to 16.4%, blowing the previous state-of-the-art out of the water.

This directly enabled further innovations like [GoogLeNet](https://arxiv.org/abs/1409.4842) and [ResNet](/02-optimization-and-regularization/03-residuals/02-residuals.ipynb), but more importantly, it shifted attention back on deep learning and created new interest in the field.

The narrative shift that occured as a result of this work was from one of skepticism about the utility of deep learning to belief that it was a viable, and even superior approach to traditional machine learning.

This narrative shift was essential to get us to the point that we're at today, and it seems that Ilya Sutskever (who co-authored AlexNet) realized how scaling laws would playout long before it reached consensus, as [discussed in this interview with Geoffrey Hinton](https://www.youtube.com/watch?v=n4IQOBka8bc).

<br />

### Narrative #2: Internet Scale Data

The [_Attention Is All You Need_](/04-transformers/01-transformer/02-transformer.ipynb) paper created a massively parallelizable architecture that enabled training on internet scale datasets.

The introduction of the Transformer alone was not what created the largest narrative shifts though.

Arguably, it was the introduction of [BERT](/04-transformers/02-bert/03-bert.ipynb) that really showed how transformers could take advantage of massive datasets scraped from the internet via pre-training and fine-tuning, which kicked off the modern trends in AI focusing on achieving general intelligence.

Because of it's transfer learning approach, BERT achieved state-of-the-art results on many NLP tasks withou training on them explicitly, showing one of the first indications of some form of _generalized_ intelligence.

The shock caused by BERT is evident in the [Google executive statement](https://x.com/TechEmails/status/1756765277478621620) claiming that BERT will replace all the 20 years of progress on the search product.

<br />

### Narrative #3: Scaling Laws

The arrow of progress defined by the improvements from [GPT-2](/04-transformers/04-gpt/01-gpt-2.pdf) to [GPT-3](/04-transformers/04-gpt/02-gpt-3.pdf) onwards created the scaling laws narrative that dominates the current public sentiment.

Importantly, OpenAI took a bet on the scaling laws early on, well before they were widely recognized as being valid[^19]. A few years ago, most people thought the scaling laws were naive.

Now, they look clear in hindisght because of the series of bets OpenAI took to validate these laws, with GPT-2 and GPT-3 further validating their hypothesis.

**Extrapolating out the progression of scaling laws correctly is challenging** - as [Zuck points out in this clip](https://www.youtube.com/watch?v=i-o5YbNfmh0), trends like these rarely continue until we reach the goal - we usually run into bottlenecks and then have to readjust strategy.

In this context, the question is how far the empirical distribution of the internet dataset will take. Framed differently - how close is the empirical distribution of the internet to the true distribution of the model of reality?

This will determine when we hit a carrying capacity on how much better our models can get by scaling parameters to train on the internet.

**This narrative is also a good indicator of how impactful narratives are in fundraising.**

The AGI narrative may be the most powerful narrative in history since it can claim that "everything else economically valuable will be solved by this problem."

Clearly, this was used effectively with the [rumored $7T OpenAI fundraising attempt](https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0) (which was of course just a rumor, but an indication of the power of the AGI narrative, since people believed it was a possibility).

<br />

[^19]: In Theil terms, you could frame this as OpenAI's "secret" or something they believe that others don't.

# 3. Inspiration

Where do the ideas that have led to breakthroughs in deep learning come from?

When we look at the history of progress, we can see several common sources of inspiration that appear frequently.

<br />

### Neuroscience

The most apparent source of direct inspiration for many advancements in deep learning is neuroscience.

The [CNN](/01-deep-neural-networks/02-cnn/01-cnn.pdf) is almost directly inspired by the visual system in the brain, and it led to significant advancements in deep learning.

Similarly, the effectiveness of [ReLU](/02-optimization-and-regularization/02-relu/) is explained in terms of the energy efficiency of sparse representations for concepts in the brain.

Other systems, like the [LSTM](/03-sequence-modeling/02-lstm/02-lstm.ipynb) and [Attention](/03-sequence-modeling/06-attention/02-attention.ipynb) mechanisms appear to draw from neuroscientific concepts (memory and attention) on a surface level, although in reality, their implementations are more motivated by the math of neural networks and engineering to specific problems rather than they are directly modeled after the brain.

For example, the LSTM design is perfectly engineering to address the vanishing & exploding gradients problem in RNNs, and it happens that a long-term memory based system is an effective way to fix this problem.

This pattern suggests that rather than taking direct inspiration from neuroscience, **deep learning may have converged on similar approaches to how nature has built intelligence in the brain, partly through first principles.**

This is a nice ex-post rationalization, but may overly construct a clean narrative that doesn't actually reflect the situation.

Additionally, early papers seem to intentionally feel pressure to fit ideas into neuroscientific and biological justifications, even where there may not have been any.

[Dropout](/02-optimization-and-regularization/04-dropout/02-dropout.ipynb) struck me as the most blatant example of this, as they explain "one possible motivation" for dropout coming from animal sexual behavior, despite their prior explanation in the paper of dropout following from a rather logical line of thinking around regularization.

This seems to an attempt to make the architecture appear to correspond with biology after it was designed, rather than it actually serving as a source for inspiration (of course, I could be wrong about this).

<br />

### Linear Algebra & Calculus

Most notably, [backpropagation](/01-deep-neural-networks/01-dnn/02-dnn.ipynb) and [LoRA](/04-transformers/05-lora) are directly inspired by the math behind neural networks.

LoRA (low-rank adaptation) is directly a manipulation on how models are trained by taking advantage of a feature of linear-algebra (decomposing weight matrices into lower dimensionality matrices with fewer trainable parameters).

Similarly, advancements like [Residuals](/02-optimization-and-regularization/03-residuals/02-residuals.ipynb) were directly motivated by the nature of gradient flows within neural networks.

<br />

### Physics & Information Theory

Notably, [VAEs](/05-image-generation/02-vae/04-vae.ipynb) and [Diffusion](/05-image-generation/03-diffusion/05-diffusion.ipynb) models take inspiration from thermodynamics - specifically Langevin dynamics, as well as probability and information theory.

These systems involve noisy sampling, and these models turn to approaches used in similarly noisy systems in the real world for inspiration

<br />

### Engineering

In practice, most of the innovations in deep learning are actually more motivated by engineering problems in neural network design, and bear only surface-level resemblance to the apparent fields of inspiration.

<br />

# 4. Intelligence

What can this progression of progress in deep learning tell us about our own intelligence?

I'll try to be purely empirical here, since it's easy to dip into unbased philosophizing with this topic given it's subjective nature.

As we've disucssed, one way way to view intelligence (motivated by the [Free Energy Principle](https://www.nature.com/articles/nrn2787)) is as a measure of our ability to model complex distributions that describe reality, and then run active inference on these models to accomplish things in the world[^20].

It seems that the combination of data about reality (dataset vs. our senses), compute (transistors vs. neurons), and energy (electricity vs. food) along with scale (parameters vs. connections), and of course, an effective learning algorithm, yields systems that appear intelligent.

Additionally, the efficacy of various inductive biases offered by different architectes may indicate something inherent about the structure of the information they're trying to model.

For example, the effectiveness of the attention mechanism raises the question of why this inductive bias alone appears to be so effective at modeling data.

If intelligence is really just a function of data, compute, energy, and training, then it seems inevitable now that digital intelligence will soon surpass us.

<br />

[^20]: This view of intelligence also paints the framework of thinking in the WaitButWhy post [The Cook and the Chef: Musk's Secret Suace](https://waitbutwhy.com/2015/11/the-cook-and-the-chef-musks-secret-sauce.html) particularly well

# 5. Future

We've now reframed the history of progress as the series of advancements that have continually raised the ceiling on the constraints governing digital intelligence.

Everything in the past that has contributed to progress has been determined by the constraints discussed above.

Importantly, nothing about this changes in the future - **these same 7 constraints will always determine where we're headed, and how close we are to AGI.[^21]**

At this point, we've solved the _theoretical problem_ of AGI, in the sense that we know exactly what would get us to AGI[^22].

This was not obvious until the past decade, where we've seen the power of how far deep learning can go.

The question is now whether we will solve the _engineering_ problem of AGI. Will we be able to keep pushing on all the constraints to keep improving digital intelligence?

Although scaling laws are currently at play and the current path forward is to acquire larger amounts of compute to train larger models, the efficacy of this approach will hit a limit in the future (it's difficult to know when).

It's possible that we may hit a bottleneck in how good models can get based on the quality of the empirical distribution of the internet, in which case we'll have to seek other sources of data.

> [!IMPORTANT]
>
> It's critical to remember that the core principle of progress in deep learning is that pushing on the 7 constraints will lead to increasingly intelligence systems.
>
> Though the scaling laws indicate that the current limiting constraints are compute and parameters, these may shift to data and energy over time, which will bring new challenges.

<br />

[^21]: This is not saying that scaling laws will get us to AGI, but that constantly pushing the constraints will get us to AGI. We may run into bottlenecks that render the scaling laws obselete at some point.
[^22]: Assuming you believe that the current systems exhibit intelligent behavior, which some people still disagree with.

# Resources

## Topics

> [!IMPORTANT]
>
> Each topic highlighted in this repository is covered in a folder linked below.
>
> In each folder, you'll find a copy of the critical papers related to the topic (`.pdf` files), along with my own breakdown of intuitions, math, and my implementation when relevant (all in the `.ipynb` file).

**1. Deep Neural Networks**

- [1.1. DNN](/01-deep-neural-networks/01-dnn/)
- [1.2. CNN](/01-deep-neural-networks/02-cnn/)
- [1.3. AlexNet](/01-deep-neural-networks/03-alex-net/)
- [1.4. UNet](/01-deep-neural-networks/04-u-net/)

**2. Optimization & Regularization**

- [2.1. Weight Decay](/02-optimization-and-regularization/01-weight-decay/)
- [2.2. ReLU](/02-optimization-and-regularization/02-relu/)
- [2.3. Residuals](/02-optimization-and-regularization/03-residuals/)
- [2.4. Dropout](/02-optimization-and-regularization/04-dropout/)
- [2.5. Batch Normalization](/02-optimization-and-regularization/05-batch-norm/)
- [2.6. Layer Normalization](/02-optimization-and-regularization/06-layer-norm/)
- [2.7. GELU](/02-optimization-and-regularization/07-gelu/)
- [2.8. Adam](/02-optimization-and-regularization/08-adam/)

**3. Sequence Modeling**

- [3.1. RNN](/03-sequence-modeling/01-rnn/)
- [3.2. LSTM](/03-sequence-modeling/02-lstm/)
- [3.3. Learning to Forget](/03-sequence-modeling/03-learning-to-forget/)
- [3.4. Word2Vec & Phrase2Vec](/03-sequence-modeling/04-word2vec/)
- [3.5. Seq2Seq](/03-sequence-modeling/05-seq2seq/)
- [3.6. Attention](/03-sequence-modeling/06-attention/)
- [3.7. Mixture of Experts](/03-sequence-modeling/07-mixture-of-experts/)

**4. Transformers**

- [4.1. Transformer](/04-transformers/01-transformer/)
- [4.2. BERT](/04-transformers/02-bert/)
- [4.3. T5](/04-transformers/03-t5)
- [4.4. GPT-2 & GPT-3](/04-transformers/04-gpt)
- [4.5. LoRA](/04-transformers/05-lora)
- [4.8. RLHF & InstructGPT](/04-transformers/06-rlhf)
- [4.9. Vision Transformer](/04-transformers/07-vision-transformer)

**5. Image Generation**

- [5.1. GANs](/05-image-generation/01-gan/)
- [5.2. VAEs](/05-image-generation/02-vae/)
- [5.3. Diffusion](/05-image-generation/03-diffusion/)
- [5.4. CLIP](/05-image-generation/05-clip/)
- [5.5. DALL E & DALL E 2](/05-image-generation/06-dall-e/)

<br />

## Implementations

I've provided my minimal implementations for many of the core topics in this repository in the `.ipynb` files for each topic.

Generally, you can find good implementations of most papers online, which means the challenge isn't figuring out reimplementation. I've included these to collect them in one place for convenience, and also to highlight simple implementations that demonstrate each concept and I've gotten trained.

I used A100's to train most of the larger models.

- [DNN](/01-deep-neural-networks/01-dnn/02-dnn.ipynb)
- [CNN](/01-deep-neural-networks/02-cnn/03-cnn.ipynb)
- [Transformer](/04-transformers/01-transformer/02-transformer.ipynb)
- [LoRA](/04-transformers/05-lora/02-lora.ipynb)
- [Vision Transformer](/04-transformers/07-vision-transformer/02-vision-transformer.ipynb)
- [GAN](/05-image-generation/01-gan/02-gan.ipynb)
- [VAE](/05-image-generation/02-vae/04-vae.ipynb)
- [Diffusion](/05-image-generation/03-diffusion/05-diffusion.ipynb)

<br />

## Papers

**Deep Neural Networks**

- **DNN** - Learning Internal Representations by Error Propagation (1987), D. E. Rumelhart et al. [[PDF]](https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf)
- **CNN** - Backpropagation Applied to Handwritten Zip Code Recognition (1989), Y. Lecun et al. [[PDF]](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)
- **LeNet** - Gradient-Based Learning Applied to Document Recognition (1998), Y. Lecun et al. [[PDF]](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
- **AlexNet** - ImageNet Classification with Deep Convolutional Networks (2012), A. Krizhevsky et al. [[PDF]](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- **U-Net** - U-Net: Convolutional Networks for Biomedical Image Segmentation (2015), O. Ronneberger et al. [[PDF]](https://arxiv.org/abs/1505.04597)

**Optimization & Regularization**

- **Weight Decay** - A Simple Weight Decay Can Improve Generalization (1991), A. Krogh and J. Hertz [[PDF]](https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf)
- **ReLU** - Deep Sparse Rectified Neural Networks (2011), X. Glorot et al. [[PDF]](https://www.researchgate.net/publication/215616967_Deep_Sparse_Rectifier_Neural_Networks)
- **Residuals** - Deep Residual Learning for Image Recognition (2015), K. He et al. [[PDF]](https://arxiv.org/pdf/1512.03385)
- **Dropout** - Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), N. Strivastava et al. [[PDF]](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
- **BatchNorm** - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015), S. Ioffe and C. Szegedy [[PDF]](https://arxiv.org/pdf/1502.03167)
- **LayerNorm** - Layer Normalization (2016), J. Lei Ba et al. [[PDF]](https://arxiv.org/pdf/1607.06450)
- **GELU** - Gaussian Error Linear Units (GELUs) (2016), D. Hendrycks and K. Gimpel [[PDF]](https://arxiv.org/pdf/1606.08415)
- **Adam** - Adam: A Method for Stochastic Optimization (2014), D. P. Kingma and J. Ba [[PDF]](https://arxiv.org/pdf/1412.6980)

**Sequence Modeling**

- **RNN** - A Learning Algorithm for Continually Running Fully Recurrent Neural Networks (1989), R. J. Williams [[PDF]](https://gwern.net/doc/ai/nn/rnn/1989-williams-2.pdf)
- **LSTM** - Long-Short Term Memory (1997), S. Hochreiter and J. Schmidhuber [[PDF]](https://www.bioinf.jku.at/publications/older/2604.pdf)
- **Learning to Forget** - Learning to Forget: Continual Prediction with LSTM (2000), F. A. Gers et al. [[PDF]](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e10f98b86797ebf6c8caea6f54cacbc5a50e8b34)
- **Word2Vec** - Efficient Estimation of Word Representations in Vector Space (2013), T. Mikolov et al. [[PDF]](https://arxiv.org/pdf/1301.3781)
- **Phrase2Vec** - Distributed Representations of Words and Phrases and their Compositionality (2013), T. Mikolov et al. [[PDF]](https://arxiv.org/pdf/1310.4546)
- **Encoder-Decoder** - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (2014), K. Cho et al. [[PDF]](https://arxiv.org/pdf/1406.1078)
- **Seq2Seq** - Sequence to Sequence Learning with Neural Networks (2014), I. Sutskever et al. [[PDF]](https://arxiv.org/pdf/1409.3215)
- **Attention** - Neural Machine Translation by Jointly Learning to Align and Translate (2014), D. Bahdanau et al. [[PDF]](https://arxiv.org/pdf/1409.0473)
- **Mixture of Experts** - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017), N. Shazeer et al. [[PDF]](https://arxiv.org/pdf/1701.06538)

**Transformers**

- **Transformer** - Attention Is All You Need (2017), A. Vaswani et al. [[PDF]](https://arxiv.org/pdf/1706.03762)
- **BERT** - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018), J. Devlin et al. [[PDF]](https://arxiv.org/pdf/1810.04805)
- **RoBERTa** - RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019), Y. Liu et al. [[PDF]](https://arxiv.org/pdf/1907.11692)
- **T5** - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019), C. Raffel et al. [[PDF]](https://arxiv.org/pdf/1910.10683)
- **GPT-2** - Language Models are Unsupervised Multitask Learners (2018), A. Radford et al. [[PDF]](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **GPT-3** - Language Models are Few-Shot Learners (2020) T. B. Brown et al. [[PDF]](https://arxiv.org/pdf/2005.14165)
- **LoRA -** LoRA: Low-Rank Adaptation of Large Language Models (2021), E. J. Hu et al. [[PDF]](https://arxiv.org/pdf/2106.09685)
- **RLHF** - Fine-Tuning Language Models From Human Preferences (2019), D. Ziegler et al. [[PDF]](https://arxiv.org/pdf/1909.08593)
- **PPO** - Proximal Policy Optimization Algorithms (2017), J. Schulman et al. [[PDF]](https://arxiv.org/pdf/1707.06347)
- **InstructGPT** - Training language models to follow instructions with human feedback (2022), L. Ouyang et al. [[PDF]](https://arxiv.org/pdf/2203.02155)
- **Helpful & Harmless** - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (2022), Y. Bai et al. [[PDF]](https://arxiv.org/pdf/2204.05862)
- **Vision Transformer** - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020), A. Dosovitskiy et al. [[PDF]](https://arxiv.org/pdf/2010.11929)

**Generative Models**

- **GAN** - Generative Adversarial Networks (2014), I. J. Goodfellow et al. [[PDF]](https://arxiv.org/pdf/1406.2661)
- **VAE** - Auto-Encoding Variational Bayes (2013), D. Kingma and M. Welling [[PDF]](https://arxiv.org/pdf/1312.6114)
- **VQ VAE** - Neural Discrete Representation Learning (2017), A. Oord et al. [[PDF]](https://arxiv.org/pdf/1711.00937)
- **VQ VAE 2** - Generating Diverse High-Fidelity Images with VQ-VAE-2 (2019), A. Razavi et al. [[PDF]](https://arxiv.org/pdf/1906.00446)
- **Diffusion** - Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015), J. Sohl-Dickstein et al. [[PDF]](https://arxiv.org/pdf/1503.03585)
- **Denoising Diffusion** - Denoising Diffusion Probabilistic Models (2020), J. Ho. et al. [[PDF]](https://arxiv.org/pdf/2006.11239)
- **Denoising Diffusion 2** - Improved Denoising Diffusion Probabilistic Models (2021), A. Nichol and P. Dhariwal [[PDF]](https://arxiv.org/pdf/2102.09672)
- **Diffusion Beats GANs** - Diffusion Models Beat GANs on Image Synthesis, P. Dhariwal and A. Nichol [[PDF]](https://arxiv.org/pdf/2105.05233)
- **CLIP** - Learning Transferable Visual Models From Natural Language Supervision (2021), A. Radford et al. [[PDF]](https://arxiv.org/pdf/2103.00020)
- **DALL E** - Zero-Shot Text-to-Image Generation (2021), A. Ramesh et al. [[PDF]](https://arxiv.org/pdf/2102.12092)
- **DALL E 2** - Hierarchical Text-Conditional Image Generation with CLIP Latents (2022), A. Ramesh et al. [[PDF]](https://arxiv.org/pdf/2204.06125)
