# deep-learning

An exploration of the entire history of deep learning progress over the past few decades that got us from simple feed-forward networks to GPT-4o, along with what we can learn from it.

Covers all the essentials in depth, including:

- Deep learning basics
- CNNs
- Optimization & regularization
- RNNs
- LSTMs
- Attention
- Transformers
- Embeddings
- GPTs
- Mixture of Experts
- Multi-modality

Each paper is in its own section with:

1. a copy of the paper itself
2. my notes highlighting important observations & explaining key concepts
3. a minimal implementation of key concepts in pytorch when relevant

Finally, I've included my observations on what this history can teach us about:

1. how progress is made in deep learning broadly
2. the constraints that govern the limit on digital intelligence
3. where the field is going
