# deep-learning

An exploration of the entire history of deep learning progress that got us from simple feed-forward networks to GPT-4o and what we can learn from it.

Covers all the essentials in depth, including:

- Deep learning basics
- CNNs
- Optimization
- Regularization
- RNNs
- LSTMs
- Attention
- Transformers
- Embeddings
- GPTs
- RLHF
- Mixture of Experts
- GANs
- Diffusion
- VQ-VAEs
- Multi-modality

Each paper is in its own section with:

1. a copy of the paper itself
2. my notes highlighting important observations & explaining key concepts
3. a minimal implementation of key concepts in pytorch when relevant

Finally, I've included my observations on what this history can teach us about:

1. how progress is made in deep learning broadly
2. the constraints that govern the limit on digital intelligence
3. where the field is going

### Philosophy

- Complete
- Minimal
- Hard & technical
- Approachable
- Up-to-date
- Preserve the richness of intuitions from learning from history
- Practicality > research frontiers
